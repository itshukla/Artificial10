Evaluating Models:@0.688403:0.948923:0.818564:0.948923:0.818564:0.935211:0.688403:0.935211:0.008148:0.007713:0.008196:0.003897:0.009114:0.008196:0.005459:0.003897:0.009114:0.009485:0.004412:0.014461:0.009436:0.009485:0.008422:0.003897:0.006828
225:@0.853958:0.949910:0.879997:0.949910:0.879997:0.936198:0.853958:0.936198:0.008680:0.008680:0.008680
1.  What are the types of evaluation techniques?:@0.138542:0.116067:0.481474:0.116067:0.481474:0.102355:0.138542:0.102355:0.008680:0.003494:0.004412:0.006473:0.015040:0.009114:0.008196:0.005459:0.004412:0.008196:0.005604:0.008422:0.004412:0.005459:0.009092:0.008422:0.004412:0.005459:0.007794:0.009469:0.008422:0.006828:0.004412:0.009436:0.005040:0.004412:0.008422:0.007713:0.008196:0.003889:0.009114:0.008196:0.005459:0.003897:0.009436:0.009114:0.004412:0.005459:0.008422:0.007440:0.009114:0.009114:0.003897:0.009485:0.009114:0.008422:0.006828:0.007214
 :@0.138542:0.142104:0.142954:0.142104:0.142954:0.128392:0.138542:0.128392:0.004412
2.  What characterises a model experiencing underfitting?:@0.138542:0.168132:0.548180:0.168132:0.548180:0.154421:0.138542:0.154421:0.008680:0.003494:0.004412:0.006473:0.015040:0.009114:0.008196:0.005459:0.004412:0.007440:0.009114:0.008196:0.005604:0.008172:0.007440:0.005459:0.008422:0.005604:0.003897:0.006828:0.008422:0.006828:0.004412:0.008196:0.004412:0.013865:0.009436:0.009485:0.008422:0.003897:0.004412:0.008422:0.007391:0.009469:0.008422:0.005604:0.003897:0.008422:0.009114:0.007440:0.003897:0.009114:0.009485:0.004412:0.009114:0.009114:0.009485:0.008422:0.005604:0.004469:0.004469:0.005459:0.005459:0.003897:0.009114:0.009485:0.007214
 :@0.138542:0.194169:0.142954:0.194169:0.142954:0.180458:0.138542:0.180458:0.004412
 Reboot:@0.203184:0.091406:0.324601:0.091406:0.324601:0.060739:0.203184:0.060739:0.009662:0.021643:0.017552:0.019871:0.019291:0.019291:0.014106
Accuracy and Error:@0.154257:0.260275:0.368496:0.260275:0.368496:0.238135:0.154257:0.238135:0.015443:0.011881:0.011881:0.014428:0.009784:0.013504:0.011881:0.012986:0.006605:0.013504:0.014428:0.014180:0.006605:0.013865:0.009784:0.009784:0.013910:0.009784
Let’s consider a scenario where, Aman and Priya went grocery shopping to buy a bag of 5 kg rice. Aman got :@0.086957:0.292466:0.891859:0.292466:0.891859:0.278068:0.086957:0.278068:0.007964:0.008843:0.005732:0.003872:0.007169:0.004514:0.007812:0.009908:0.009570:0.007169:0.004092:0.009959:0.008843:0.005884:0.004513:0.008606:0.004513:0.007169:0.007812:0.008843:0.009570:0.008606:0.005884:0.004092:0.009908:0.004511:0.012225:0.009570:0.008843:0.005884:0.008843:0.003669:0.004531:0.010906:0.014558:0.008606:0.009570:0.004518:0.008606:0.009570:0.009959:0.004509:0.009469:0.005884:0.004092:0.008184:0.008606:0.004509:0.012225:0.008843:0.009570:0.005732:0.004506:0.009959:0.005884:0.009908:0.007812:0.008843:0.005884:0.008184:0.004498:0.007169:0.009570:0.009908:0.009942:0.009942:0.004092:0.009570:0.009959:0.004516:0.005732:0.009908:0.004513:0.009942:0.009570:0.008184:0.004513:0.008606:0.004513:0.009942:0.008606:0.009959:0.004509:0.009908:0.005292:0.004514:0.009114:0.004518:0.008403:0.009959:0.004516:0.005884:0.004092:0.007812:0.008843:0.003669:0.004509:0.010906:0.014558:0.008606:0.009570:0.004516:0.009959:0.009908:0.005732:0.004633
 :@0.321330:0.292642:0.325997:0.292642:0.325997:0.278182:0.321330:0.278182:0.004667
`:@0.891738:0.290973:0.900192:0.290973:0.900192:0.275884:0.891738:0.275884:0.008454
450 :@0.900192:0.292466:0.932166:0.292466:0.932166:0.278068:0.900192:0.278068:0.009114:0.009114:0.009114:0.004633
and Priya got :@0.086957:0.310984:0.190824:0.310984:0.190824:0.296587:0.086957:0.296587:0.008606:0.009570:0.009959:0.004633:0.009469:0.005884:0.004092:0.008184:0.008606:0.004633:0.009959:0.009908:0.005732:0.004633
`:@0.190800:0.309492:0.199254:0.309492:0.199254:0.294403:0.190800:0.294403:0.008454
500. The actual price of the rice was  550.:@0.199254:0.310984:0.508985:0.310984:0.508985:0.296587:0.199254:0.296587:0.009114:0.009114:0.009114:0.003669:0.004633:0.008860:0.009570:0.008843:0.004633:0.008606:0.007812:0.005732:0.009570:0.008606:0.004092:0.004633:0.009942:0.005884:0.004092:0.007812:0.008843:0.004633:0.009908:0.005292:0.004633:0.005732:0.009570:0.008843:0.004633:0.005884:0.004092:0.007812:0.008843:0.004633:0.012225:0.008606:0.007169:0.004633:0.008410:0.009114:0.009114:0.009114:0.003669
`:@0.469521:0.309492:0.477976:0.309492:0.477976:0.294403:0.469521:0.294403:0.008454
 :@0.094734:0.333002:0.099367:0.333002:0.099367:0.318605:0.094734:0.318605:0.004633
•:@0.086957:0.331302:0.094734:0.331302:0.094734:0.315423:0.086957:0.315423:0.007778
Who is more accurate? Aman or Priya?  :@0.105215:0.333002:0.401684:0.333002:0.401684:0.318605:0.105215:0.318605:0.015792:0.009570:0.009908:0.004633:0.004092:0.007169:0.004633:0.014558:0.009908:0.005884:0.008843:0.004633:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.005732:0.008843:0.007575:0.004633:0.010906:0.014558:0.008606:0.009570:0.004633:0.009908:0.005884:0.004633:0.009469:0.005884:0.004092:0.008184:0.008606:0.007575:0.004633:0.004633
 :@0.401654:0.333002:0.406287:0.333002:0.406287:0.318605:0.401654:0.318605:0.004633
Priya was more accurate, as her estimate is closer to the actual price.:@0.105215:0.351521:0.613831:0.351521:0.613831:0.337124:0.105215:0.337124:0.009469:0.005884:0.004092:0.008184:0.008606:0.004633:0.012225:0.008606:0.007169:0.004633:0.014558:0.009908:0.005884:0.008843:0.004633:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.005732:0.008843:0.003669:0.004633:0.008606:0.007169:0.004633:0.009570:0.008843:0.005884:0.004633:0.008843:0.007169:0.005732:0.004092:0.014558:0.008606:0.005732:0.008843:0.004633:0.004092:0.007169:0.004633:0.007812:0.004092:0.009908:0.007169:0.008843:0.005884:0.004633:0.005732:0.009908:0.004633:0.005732:0.009570:0.008843:0.004633:0.008606:0.007812:0.005732:0.009570:0.008606:0.004092:0.004633:0.009942:0.005884:0.004092:0.007812:0.008843:0.003669
 :@0.094734:0.373539:0.099367:0.373539:0.099367:0.359142:0.094734:0.359142:0.004633
•:@0.086957:0.371839:0.094734:0.371839:0.094734:0.355959:0.086957:0.355959:0.007778
How much is the error for both Aman and Priya in estimating the price of a bag of rice?:@0.105215:0.373539:0.754761:0.373539:0.754761:0.359142:0.105215:0.359142:0.012005:0.009908:0.012225:0.004633:0.014558:0.009570:0.007812:0.009570:0.004633:0.004092:0.007169:0.004633:0.005732:0.009570:0.008843:0.004633:0.008843:0.005884:0.005884:0.009908:0.005884:0.004633:0.005292:0.009908:0.005884:0.004633:0.009942:0.009908:0.005732:0.009570:0.004633:0.010906:0.014558:0.008606:0.009570:0.004633:0.008606:0.009570:0.009959:0.004633:0.009469:0.005884:0.004092:0.008184:0.008606:0.004633:0.004092:0.009570:0.004633:0.008843:0.007169:0.005732:0.004092:0.014558:0.008606:0.005732:0.004092:0.009570:0.009959:0.004633:0.005732:0.009570:0.008843:0.004633:0.009942:0.005884:0.004092:0.007812:0.008843:0.004633:0.009908:0.005292:0.004633:0.008606:0.004633:0.009942:0.008606:0.009959:0.004633:0.009908:0.005292:0.004633:0.005884:0.004092:0.007812:0.008843:0.007575
 :@0.754671:0.373539:0.759303:0.373539:0.759303:0.359142:0.754671:0.359142:0.004633
Error is the difference between the measured value and true value.:@0.105215:0.392057:0.599324:0.392057:0.599324:0.377660:0.105215:0.377660:0.008556:0.005884:0.005884:0.009908:0.005884:0.004633:0.004092:0.007169:0.004633:0.005732:0.009570:0.008843:0.004633:0.009959:0.004092:0.005292:0.005292:0.008843:0.005884:0.008843:0.009570:0.007812:0.008843:0.004633:0.009942:0.008843:0.005732:0.012225:0.008843:0.008843:0.009570:0.004633:0.005732:0.009570:0.008843:0.004633:0.014558:0.008843:0.008606:0.007169:0.009570:0.005884:0.008843:0.009959:0.004633:0.008099:0.008606:0.004092:0.009570:0.008843:0.004633:0.008606:0.009570:0.009959:0.004633:0.005732:0.005884:0.009570:0.008843:0.004633:0.008099:0.008606:0.004092:0.009570:0.008843:0.003669
 :@0.599235:0.392057:0.603868:0.392057:0.603868:0.377660:0.599235:0.377660:0.004633
Aman’s Error is  100 (550-400):@0.105215:0.411810:0.333246:0.411810:0.333246:0.397413:0.105215:0.397413:0.010906:0.014558:0.008606:0.009570:0.003872:0.007169:0.004633:0.008556:0.005884:0.005884:0.009908:0.005884:0.004633:0.004092:0.007169:0.004633:0.008444:0.009114:0.009114:0.009114:0.004633:0.005106:0.009114:0.009114:0.009114:0.006763:0.009114:0.009114:0.009114:0.005106
`:@0.221162:0.410318:0.229616:0.410318:0.229616:0.395229:0.221162:0.395229:0.008454
 :@0.333245:0.411810:0.337878:0.411810:0.337878:0.397413:0.333245:0.397413:0.004633
Priya’s Error is  50 (550-500):@0.105215:0.431563:0.316719:0.431563:0.316719:0.417166:0.105215:0.417166:0.009469:0.005884:0.004092:0.008184:0.008606:0.003872:0.007169:0.004633:0.008556:0.005884:0.005884:0.009908:0.005884:0.004633:0.004092:0.007169:0.004633:0.008436:0.009114:0.009114:0.004633:0.005106:0.009114:0.009114:0.009114:0.006763:0.009114:0.009114:0.009114:0.005106
`:@0.213748:0.430071:0.222202:0.430071:0.222202:0.414982:0.213748:0.414982:0.008454
 :@0.316716:0.431563:0.321349:0.431563:0.321349:0.417166:0.316716:0.417166:0.004633
Here, Priya is more accurate as her error is less than Aman’s error.:@0.105215:0.451317:0.592848:0.451317:0.592848:0.436919:0.105215:0.436919:0.012005:0.008843:0.005884:0.008843:0.003669:0.004633:0.009469:0.005884:0.004092:0.008184:0.008606:0.004633:0.004092:0.007169:0.004633:0.014558:0.009908:0.005884:0.008843:0.004633:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.005732:0.008843:0.004633:0.008606:0.007169:0.004633:0.009570:0.008843:0.005884:0.004633:0.008843:0.005884:0.005884:0.009908:0.005884:0.004633:0.004092:0.007169:0.004633:0.004092:0.008843:0.007169:0.007169:0.004633:0.005732:0.009570:0.008606:0.009570:0.004633:0.010906:0.014558:0.008606:0.009570:0.003872:0.007169:0.004633:0.008843:0.005884:0.005884:0.009908:0.005884:0.003669
The term :@0.086957:0.473335:0.160179:0.473335:0.160179:0.458938:0.086957:0.458938:0.008860:0.009570:0.008843:0.006300:0.005732:0.008843:0.005884:0.014558:0.004633
Accuracy:@0.161846:0.473511:0.233232:0.473511:0.233232:0.459051:0.161846:0.459051:0.011886:0.008116:0.008116:0.010229:0.006729:0.009097:0.008116:0.009097
 is defined as the evaluation metric that measures the total number of predictions that are :@0.233235:0.473335:0.932192:0.473335:0.932192:0.458938:0.233235:0.458938:0.006303:0.004092:0.007169:0.006312:0.009959:0.008843:0.004692:0.004692:0.009570:0.008843:0.009959:0.006290:0.008606:0.007169:0.006305:0.005732:0.009570:0.008843:0.006298:0.008843:0.008099:0.008606:0.004092:0.009570:0.008606:0.005732:0.004092:0.009908:0.009570:0.006302:0.014558:0.008843:0.005732:0.005884:0.004092:0.007812:0.006302:0.005732:0.009570:0.008606:0.005732:0.006293:0.014558:0.008843:0.008606:0.007169:0.009570:0.005884:0.008843:0.007169:0.006308:0.005732:0.009570:0.008843:0.006303:0.005732:0.009908:0.005732:0.008606:0.004092:0.006297:0.009570:0.009570:0.014558:0.009942:0.008843:0.005884:0.006297:0.009908:0.005292:0.006303:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009908:0.009570:0.007169:0.006297:0.005732:0.009570:0.008606:0.005732:0.006295:0.008606:0.005884:0.008843:0.004633
correct by the model. It means how close the prediction is to the true value. The accuracy of the model and the :@0.086957:0.491853:0.932171:0.491853:0.932171:0.477456:0.086957:0.477456:0.007812:0.009908:0.005884:0.005884:0.008843:0.007812:0.005732:0.005340:0.009942:0.008184:0.005355:0.005732:0.009570:0.008843:0.005353:0.014558:0.009908:0.009959:0.008843:0.004092:0.003669:0.005358:0.004498:0.005732:0.005358:0.014558:0.008843:0.008606:0.009570:0.007169:0.005362:0.009570:0.009908:0.012225:0.005350:0.007812:0.004092:0.009908:0.007169:0.008843:0.005363:0.005732:0.009570:0.008843:0.005353:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009900:0.009570:0.005357:0.004092:0.007169:0.005367:0.005732:0.009908:0.005355:0.005732:0.009570:0.008843:0.005353:0.005732:0.005884:0.009570:0.008843:0.005348:0.008099:0.008606:0.004092:0.009570:0.008843:0.003669:0.005352:0.008860:0.009570:0.008843:0.005355:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.007812:0.008184:0.005338:0.009908:0.005292:0.005357:0.005732:0.009570:0.008843:0.005353:0.014558:0.009908:0.009959:0.008843:0.004092:0.005363:0.008606:0.009570:0.009959:0.005350:0.005732:0.009570:0.008843:0.004633
performance of the model is :@0.086957:0.510372:0.303928:0.510372:0.303928:0.495975:0.086957:0.495975:0.009942:0.008843:0.005884:0.005292:0.009908:0.005884:0.014558:0.008606:0.009570:0.007812:0.008843:0.004795:0.009908:0.005292:0.004810:0.005732:0.009570:0.008843:0.004807:0.014558:0.009908:0.009959:0.008843:0.004092:0.004817:0.004092:0.007169:0.004633
directly proportional:@0.304116:0.510548:0.470004:0.510548:0.470004:0.496088:0.304116:0.496088:0.010466:0.004802:0.006669:0.009147:0.008116:0.006577:0.004802:0.009097:0.004853:0.010483:0.006665:0.010331:0.010483:0.010331:0.007228:0.006577:0.004802:0.010331:0.010229:0.009097:0.004802
, which means better the performance of the model,  leads to :@0.470019:0.510372:0.932172:0.510372:0.932172:0.495975:0.470019:0.495975:0.003669:0.004809:0.012225:0.009570:0.004092:0.007812:0.009570:0.004805:0.014558:0.008843:0.008606:0.009570:0.007169:0.004817:0.009942:0.008843:0.005732:0.005732:0.008843:0.005884:0.004799:0.005732:0.009570:0.008843:0.004807:0.009942:0.008843:0.005884:0.005292:0.009908:0.005884:0.014558:0.008606:0.009570:0.007812:0.008843:0.004795:0.009908:0.005292:0.004810:0.005732:0.009570:0.008843:0.004809:0.014558:0.009908:0.009959:0.008843:0.004092:0.003669:0.004812:0.004812:0.004092:0.008843:0.008606:0.009959:0.007169:0.004814:0.005732:0.009908:0.004633
higher accuracy in predictions. :@0.086957:0.528890:0.318024:0.528890:0.318024:0.514493:0.086957:0.514493:0.009570:0.004092:0.009959:0.009570:0.008843:0.005884:0.004633:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.007812:0.008184:0.004633:0.004092:0.009570:0.004633:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009908:0.009570:0.007169:0.003669:0.004633
The term :@0.086957:0.550908:0.157178:0.550908:0.157178:0.536511:0.086957:0.536511:0.008860:0.009570:0.008843:0.003299:0.005732:0.008843:0.005884:0.014558:0.004633
Error :@0.155843:0.551084:0.199960:0.551084:0.199960:0.536625:0.155843:0.536625:0.008995:0.006729:0.006665:0.010331:0.006729:0.004667
means the action that is inaccurate or wrong. It refers to the difference between a model’s prediction :@0.198624:0.550908:0.932187:0.550908:0.932187:0.536511:0.198624:0.536511:0.014558:0.008843:0.008606:0.009570:0.007169:0.003307:0.005732:0.009570:0.008843:0.003297:0.008606:0.007812:0.005732:0.004092:0.009908:0.009570:0.003295:0.005732:0.009570:0.008606:0.005732:0.003292:0.004092:0.007169:0.003311:0.004092:0.009570:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.005732:0.008843:0.003287:0.009908:0.005884:0.003295:0.012225:0.005884:0.009908:0.009570:0.009959:0.003669:0.003282:0.004498:0.005732:0.003302:0.005884:0.008843:0.005292:0.008843:0.005884:0.007169:0.003294:0.005732:0.009908:0.003299:0.005732:0.009570:0.008843:0.003300:0.009959:0.004092:0.005258:0.005258:0.008843:0.005884:0.008843:0.009570:0.007812:0.008843:0.003292:0.009942:0.008843:0.005732:0.012225:0.008843:0.008843:0.009570:0.003289:0.008606:0.003299:0.014558:0.009908:0.009959:0.008843:0.004092:0.003872:0.007169:0.003312:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009908:0.009570:0.004633
and the actual outcome. It quantifies how often the model makes mistakes. Based on the error, we choose the :@0.086957:0.569427:0.932173:0.569427:0.932173:0.555030:0.086957:0.555030:0.008606:0.009570:0.009959:0.005884:0.005732:0.009570:0.008843:0.005886:0.008606:0.007812:0.005732:0.009570:0.008606:0.004092:0.005882:0.009908:0.009570:0.005732:0.007812:0.009908:0.014558:0.008843:0.003669:0.005886:0.004498:0.005732:0.005891:0.009959:0.009570:0.008606:0.009570:0.005732:0.004092:0.004692:0.004692:0.008843:0.007169:0.005882:0.009570:0.009908:0.012225:0.005882:0.009908:0.005292:0.005732:0.008843:0.009570:0.005886:0.005732:0.009570:0.008843:0.005886:0.014558:0.009908:0.009959:0.008843:0.004092:0.005896:0.014558:0.008606:0.008403:0.008843:0.007169:0.005899:0.014558:0.004092:0.007169:0.005732:0.008606:0.008403:0.008843:0.007169:0.003669:0.005901:0.009688:0.008606:0.007169:0.008843:0.009959:0.005894:0.009908:0.009570:0.005887:0.005732:0.009570:0.008843:0.005887:0.008843:0.005884:0.005884:0.009908:0.005884:0.003669:0.005869:0.012225:0.008843:0.005884:0.007812:0.009570:0.009908:0.009908:0.007169:0.008843:0.005889:0.005732:0.009570:0.008843:0.004633
machine learning model that has the best performance for a specific dataset. Low error in a model 's performance :@0.086957:0.587945:0.932186:0.587945:0.932186:0.573548:0.086957:0.573548:0.014558:0.008606:0.007812:0.009570:0.004092:0.009570:0.008843:0.004351:0.004092:0.008843:0.008606:0.005884:0.009562:0.004092:0.009570:0.009959:0.004351:0.014558:0.009908:0.009959:0.008843:0.004092:0.004354:0.005732:0.009570:0.008606:0.005732:0.004342:0.009570:0.008606:0.007169:0.004351:0.005732:0.009570:0.008843:0.004345:0.009942:0.008843:0.007169:0.005732:0.004350:0.009942:0.008843:0.005884:0.005292:0.009908:0.005884:0.014558:0.008606:0.009570:0.007812:0.008843:0.004333:0.005292:0.009908:0.005884:0.004344:0.008606:0.004347:0.007169:0.009942:0.008843:0.007812:0.004092:0.004692:0.004692:0.007812:0.004347:0.009959:0.008606:0.005732:0.008606:0.007169:0.008843:0.005732:0.003669:0.004337:0.007964:0.009908:0.012225:0.004339:0.008843:0.005884:0.005884:0.009908:0.005884:0.004332:0.004092:0.009570:0.004352:0.008606:0.004345:0.014558:0.009908:0.009959:0.008843:0.004092:0.004356:0.003889:0.007169:0.004356:0.009942:0.008843:0.005884:0.005292:0.009908:0.005884:0.014558:0.008606:0.009570:0.007812:0.008843:0.004633
signifies precise and reliable predictions. :@0.086957:0.606464:0.392184:0.606464:0.392184:0.592067:0.086957:0.592067:0.007169:0.004092:0.009959:0.009570:0.004092:0.004692:0.004692:0.008843:0.007169:0.004633:0.009942:0.005884:0.008843:0.007812:0.004092:0.007169:0.008843:0.004633:0.008606:0.009570:0.009959:0.004633:0.005884:0.008843:0.004092:0.004092:0.008606:0.009942:0.004092:0.008843:0.004633:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009908:0.009570:0.007169:0.003669:0.004633
Based on the present error, the AI model :@0.442993:0.631748:0.734533:0.631748:0.734533:0.618117:0.442993:0.618117:0.009173:0.008149:0.006788:0.008373:0.009429:0.004386:0.009381:0.009061:0.004386:0.005427:0.009061:0.008373:0.004386:0.009413:0.005571:0.008373:0.006788:0.008373:0.009061:0.005427:0.004386:0.008373:0.005571:0.005571:0.009381:0.005571:0.003474:0.004386:0.005427:0.009061:0.008373:0.004386:0.010326:0.004258:0.004386:0.013784:0.009381:0.009429:0.008373:0.003874:0.004386
parameters are fine tuned to reduce further error:@0.413793:0.646476:0.759411:0.646476:0.759411:0.632845:0.413793:0.632845:0.009413:0.008149:0.005571:0.008149:0.013784:0.008373:0.005427:0.008373:0.005571:0.006788:0.004386:0.008149:0.005571:0.008373:0.004386:0.004443:0.004443:0.009061:0.008373:0.004386:0.005427:0.009061:0.009061:0.008373:0.009429:0.004386:0.005427:0.009381:0.004386:0.005571:0.008373:0.009429:0.009061:0.007396:0.008373:0.004386:0.005011:0.009061:0.005571:0.005427:0.009061:0.008373:0.005571:0.004386:0.008373:0.005571:0.005571:0.009381:0.005571
Input Data:@0.198588:0.713649:0.273142:0.713649:0.273142:0.700017:0.198588:0.700017:0.004258:0.009061:0.009413:0.009061:0.005427:0.004386:0.011222:0.008149:0.005427:0.008149
AI Model:@0.374356:0.713649:0.438760:0.713649:0.438760:0.700017:0.374356:0.700017:0.010326:0.004258:0.004386:0.014376:0.009381:0.009429:0.008373:0.003874
Error:@0.761633:0.712754:0.795828:0.712754:0.795828:0.699123:0.761633:0.699123:0.008100:0.005571:0.005571:0.009381:0.005571
=:@0.691481:0.713122:0.702431:0.713122:0.702431:0.699491:0.691481:0.699491:0.010950
Predicted Value:@0.529947:0.688488:0.639399:0.688488:0.639399:0.674856:0.529947:0.674856:0.008965:0.005571:0.008373:0.009429:0.003874:0.007396:0.005427:0.008373:0.009429:0.004386:0.008771:0.008149:0.003874:0.009061:0.008373
Actual Value:@0.541247:0.736082:0.628094:0.736082:0.628094:0.722450:0.541247:0.722450:0.010326:0.007396:0.005427:0.009061:0.008149:0.003874:0.004386:0.008771:0.008149:0.003874:0.009061:0.008373
less:@0.571744:0.711424:0.597566:0.711424:0.597566:0.697793:0.571744:0.697793:0.003874:0.008373:0.006788:0.006788
Note, high accuracy in a model indicates better model performance, but it :@0.086957:0.774004:0.646431:0.774004:0.646431:0.759607:0.086957:0.759607:0.012647:0.009908:0.005732:0.008843:0.003669:0.004986:0.009570:0.004092:0.009959:0.009570:0.004990:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.007812:0.008184:0.004971:0.004092:0.009570:0.004995:0.008606:0.004988:0.014558:0.009908:0.009959:0.008843:0.004092:0.004991:0.004092:0.009570:0.009959:0.004092:0.007812:0.008606:0.005732:0.008843:0.007169:0.004993:0.009942:0.008843:0.005732:0.005732:0.008843:0.005884:0.004978:0.014558:0.009908:0.009959:0.008843:0.004092:0.004996:0.009942:0.008843:0.005884:0.005292:0.009908:0.005884:0.014558:0.008606:0.009570:0.007812:0.008843:0.003669:0.004974:0.009942:0.009570:0.005732:0.004986:0.004092:0.005732:0.004633
may not reflect the true scenario, especially with an imbalance dataset.:@0.086957:0.792523:0.611973:0.792523:0.611973:0.778126:0.086957:0.778126:0.014558:0.008606:0.008184:0.004633:0.009570:0.009908:0.005732:0.004633:0.005884:0.008843:0.004692:0.004692:0.008843:0.007812:0.005732:0.004633:0.005732:0.009570:0.008843:0.004633:0.005732:0.005884:0.009570:0.008843:0.004633:0.007169:0.007812:0.008843:0.009570:0.008606:0.005884:0.004092:0.009908:0.003669:0.004633:0.008843:0.007169:0.009942:0.008843:0.007812:0.004092:0.008606:0.004092:0.004092:0.008184:0.004633:0.012225:0.004092:0.005732:0.009570:0.004633:0.008606:0.009570:0.004633:0.004092:0.014558:0.009942:0.008606:0.004092:0.008606:0.009570:0.007812:0.008843:0.004633:0.009959:0.008606:0.005732:0.008606:0.007169:0.008843:0.005732:0.003669
For  example,  you’re  training  a  model  to  predict  the  approval  of  loan :@0.086957:0.814541:0.646444:0.814541:0.646444:0.800144:0.086957:0.800144:0.008251:0.009908:0.005884:0.004633:0.004484:0.008843:0.007761:0.008606:0.014558:0.009942:0.004092:0.008843:0.003669:0.004633:0.004484:0.008184:0.009908:0.009570:0.003872:0.005884:0.008843:0.004633:0.004476:0.005732:0.005884:0.008606:0.004092:0.009570:0.004092:0.009570:0.009959:0.004633:0.004484:0.008606:0.004633:0.004482:0.014558:0.009908:0.009959:0.008843:0.004092:0.004633:0.004491:0.005732:0.009908:0.004633:0.004484:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004633:0.004476:0.005732:0.009570:0.008843:0.004633:0.004481:0.008606:0.009942:0.009942:0.005884:0.009908:0.008099:0.008606:0.004092:0.004633:0.004472:0.009908:0.005292:0.004633:0.004486:0.004092:0.009908:0.008606:0.009570:0.004633
(Classification Task) :@0.086957:0.833060:0.234269:0.833060:0.234269:0.818662:0.086957:0.818662:0.005106:0.010466:0.004092:0.008606:0.007169:0.007169:0.004092:0.004692:0.004692:0.007812:0.008606:0.005732:0.004092:0.009908:0.009570:0.004633:0.006958:0.008606:0.007169:0.008403:0.005106:0.004633
 :@0.094734:0.855078:0.099367:0.855078:0.099367:0.840680:0.094734:0.840680:0.004633
•:@0.086957:0.853378:0.094734:0.853378:0.094734:0.837498:0.086957:0.837498:0.007778
Error::@0.105215:0.855254:0.149929:0.855254:0.149929:0.840794:0.105215:0.840794:0.008995:0.006729:0.006665:0.010331:0.007411:0.004582
 If the model predicts that the loan will not be approved but actually :@0.149929:0.855078:0.646434:0.855078:0.646434:0.840680:0.149929:0.840680:0.003356:0.004498:0.005292:0.003355:0.005732:0.009570:0.008843:0.003351:0.014558:0.009908:0.009959:0.008843:0.004092:0.003355:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.007169:0.003349:0.005732:0.009570:0.008606:0.005732:0.003346:0.005732:0.009570:0.008843:0.003351:0.004092:0.009908:0.008606:0.009570:0.003353:0.012225:0.004092:0.004092:0.004092:0.003358:0.009570:0.009908:0.005732:0.003351:0.009942:0.008843:0.003353:0.008606:0.009942:0.009942:0.005884:0.009895:0.008099:0.008843:0.009959:0.003353:0.009942:0.009570:0.005732:0.003349:0.008606:0.007812:0.005732:0.009570:0.008606:0.004092:0.004092:0.008184:0.004633
it’s approved by the bank, that’s an error. The error is the measurement :@0.105215:0.873596:0.646435:0.873596:0.646435:0.859199:0.105215:0.859199:0.004092:0.005732:0.003872:0.007169:0.005171:0.008606:0.009942:0.009942:0.005884:0.009908:0.008099:0.008843:0.009959:0.005147:0.009942:0.008184:0.005160:0.005732:0.009570:0.008843:0.005159:0.009942:0.008606:0.009570:0.008403:0.003669:0.005155:0.005732:0.009570:0.008606:0.005732:0.003872:0.007169:0.005160:0.008606:0.009570:0.005159:0.008843:0.005884:0.005884:0.009908:0.005884:0.003669:0.005142:0.008860:0.009570:0.008843:0.005160:0.008843:0.005884:0.005884:0.009908:0.005884:0.005143:0.004092:0.007169:0.005174:0.005732:0.009570:0.008843:0.005159:0.014558:0.008843:0.008606:0.007169:0.009570:0.005884:0.008843:0.014558:0.008843:0.009570:0.005732:0.004633
of the difference between the prediction and the actual outcome.:@0.105215:0.892115:0.591360:0.892115:0.591360:0.877718:0.105215:0.877718:0.009908:0.005292:0.004633:0.005732:0.009570:0.008843:0.004633:0.009959:0.004092:0.005292:0.005292:0.008843:0.005884:0.008843:0.009570:0.007812:0.008843:0.004633:0.009942:0.008843:0.005732:0.012225:0.008843:0.008843:0.009570:0.004633:0.005732:0.009570:0.008843:0.004633:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009908:0.009570:0.004633:0.008606:0.009570:0.009959:0.004633:0.005732:0.009570:0.008843:0.004633:0.008606:0.007812:0.005732:0.009570:0.008606:0.004092:0.004633:0.009908:0.009570:0.005732:0.007812:0.009908:0.014558:0.008843:0.003669