T:@0.180169:0.948923:0.188607:0.948923:0.188607:0.935211:0.180169:0.935211:0.008438
ouchpad Artificial Intelligence (Ver. 3.0):@0.187317:0.948923:0.463822:0.948083:0.463822:0.934372:0.187317:0.935211:0.009436:0.009114:0.007440:0.009114:0.009469:0.008196:0.009485:0.004412:0.010386:0.005604:0.005459:0.003897:0.004469:0.004469:0.007440:0.003897:0.008196:0.003897:0.004412:0.004283:0.009114:0.005459:0.008422:0.003897:0.003897:0.003897:0.009485:0.008422:0.009114:0.007440:0.008422:0.004412:0.004863:0.008952:0.008422:0.005604:0.003494:0.004412:0.008680:0.003494:0.008665:-0.373357
-X:@0.463821:0.948260:0.480037:0.948926:0.480037:0.935215:0.463821:0.934548:0.006715:-0.386033
248:@0.118726:0.949910:0.144765:0.949910:0.144765:0.936198:0.118726:0.936198:0.008680:0.008680:0.008680
 :@0.072464:0.068212:0.076655:0.068212:0.076655:0.055186:0.072464:0.055186:0.004192
8.  The Train-test split technique is used to evaluate the performance of the model by dividing the dataset into two :@0.101254:0.068212:0.917252:0.068212:0.917252:0.055186:0.101254:0.055186:0.008246:0.003320:0.004192:0.005370:0.008016:0.008659:0.008001:0.006388:0.006685:0.005324:0.007787:0.003702:0.008659:0.006119:0.005186:0.008001:0.006486:0.005186:0.006388:0.006486:0.008995:0.003702:0.003702:0.005186:0.006399:0.005186:0.008001:0.007068:0.008659:0.008659:0.003702:0.009010:0.008659:0.008001:0.006388:0.003702:0.006486:0.006399:0.008659:0.006486:0.008001:0.009010:0.006393:0.005186:0.008965:0.006388:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.008001:0.006382:0.005186:0.008659:0.008001:0.006387:0.008995:0.008001:0.005324:0.004788:0.008965:0.005324:0.013171:0.007787:0.008659:0.007068:0.008001:0.006376:0.008965:0.004788:0.006390:0.005186:0.008659:0.008001:0.006387:0.013171:0.008965:0.009010:0.008001:0.003702:0.006396:0.008995:0.007404:0.006387:0.009010:0.003702:0.007328:0.003702:0.009010:0.003702:0.008659:0.009010:0.006393:0.005186:0.008659:0.008001:0.006387:0.009010:0.007787:0.005186:0.007787:0.006486:0.008001:0.005186:0.006382:0.003702:0.008659:0.005186:0.008965:0.006390:0.005186:0.011060:0.008965:0.004192
subsets: a :@0.122383:0.087965:0.192172:0.087965:0.192172:0.074940:0.122383:0.074940:0.006486:0.008659:0.008995:0.006486:0.008001:0.005186:0.006486:0.003320:0.004192:0.007787:0.004192
……………………….:@0.192172:0.087965:0.296412:0.087965:0.296412:0.074940:0.192172:0.074940:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.003320
 subset and a :@0.296397:0.087965:0.390219:0.087965:0.390219:0.074940:0.296397:0.074940:0.004192:0.006486:0.008659:0.008995:0.006486:0.008001:0.005186:0.004192:0.007787:0.008659:0.009010:0.004192:0.007787:0.004192
……………………….:@0.390209:0.087965:0.494449:0.087965:0.494449:0.074940:0.390209:0.074940:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.003320
 subset.:@0.494433:0.087965:0.545758:0.087965:0.545758:0.074940:0.494433:0.074940:0.004192:0.006486:0.008659:0.008995:0.006486:0.008001:0.005186:0.003320
 :@0.072464:0.113843:0.076655:0.113843:0.076655:0.100817:0.072464:0.100817:0.004192
9.  Low error signifies precise and :@0.101254:0.113843:0.331075:0.113843:0.331075:0.100817:0.101254:0.100817:0.008246:0.003320:0.004192:0.005370:0.007205:0.008965:0.011060:0.004192:0.008001:0.005324:0.005324:0.008965:0.005324:0.004192:0.006486:0.003702:0.009010:0.008659:0.003702:0.004788:0.003702:0.008001:0.006486:0.004192:0.008995:0.005324:0.008001:0.007068:0.003702:0.006486:0.008001:0.004192:0.007787:0.008659:0.009010:0.004192
……………………….:@0.331049:0.113843:0.435289:0.113843:0.435289:0.100817:0.331049:0.100817:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.003320
 predictions.:@0.435273:0.113843:0.517882:0.113843:0.517882:0.100817:0.435273:0.100817:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.003702:0.008965:0.008659:0.006486:0.003320
 :@0.072464:0.139720:0.076655:0.139720:0.076655:0.126694:0.072464:0.126694:0.004192
10.  The model must achieve a balance of :@0.093009:0.139720:0.372959:0.139720:0.372959:0.126694:0.093009:0.126694:0.008246:0.008246:0.003320:0.004192:0.005370:0.008016:0.008659:0.008001:0.003544:0.013171:0.008965:0.009010:0.008001:0.003702:0.003549:0.013171:0.008659:0.006486:0.005186:0.003554:0.007787:0.007068:0.008659:0.003702:0.008001:0.007328:0.008001:0.003548:0.007787:0.003545:0.008995:0.007787:0.003702:0.007787:0.008659:0.007068:0.008001:0.003538:0.008965:0.004788:0.004192
……………………….:@0.372313:0.139720:0.476553:0.139720:0.476553:0.126694:0.372313:0.126694:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.003320
 and :@0.476537:0.139720:0.509734:0.139720:0.509734:0.126694:0.476537:0.126694:0.003549:0.007787:0.008659:0.009010:0.004192
……………………….:@0.509081:0.139720:0.613321:0.139720:0.613321:0.126694:0.509081:0.126694:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.011213:0.003320
 to perform optimally in real-world scenarios.:@0.613305:0.139720:0.913046:0.139720:0.913046:0.126694:0.613305:0.126694:0.003548:0.005186:0.008965:0.003544:0.008995:0.008001:0.005324:0.004788:0.008965:0.005324:0.013171:0.003537:0.008965:0.008995:0.005186:0.003702:0.013171:0.007787:0.003702:0.003702:0.007404:0.003552:0.003702:0.008659:0.003549:0.005324:0.008001:0.007787:0.003702:0.006119:0.011060:0.008965:0.005324:0.003702:0.009010:0.003528:0.006486:0.007068:0.008001:0.008659:0.007787:0.005324:0.003702:0.008965:0.006486:0.003320
C.  Match the following::@0.072464:0.170886:0.256828:0.170886:0.256828:0.156439:0.072464:0.156439:0.010500:0.004075:0.004650:0.006543:0.015623:0.008826:0.005965:0.007947:0.009841:0.004650:0.006104:0.009841:0.008978:0.004650:0.005833:0.010094:0.004413:0.004413:0.010094:0.012783:0.004413:0.009857:0.010196:0.004075
 :@0.072464:0.192329:0.076655:0.192329:0.076655:0.179303:0.072464:0.179303:0.004192
1.  Training Set    :@0.101254:0.192329:0.215784:0.192329:0.215784:0.179303:0.101254:0.179303:0.008246:0.003320:0.004192:0.005370:0.006685:0.005324:0.007787:0.003702:0.008659:0.003702:0.008659:0.009010:0.004192:0.008123:0.008001:0.005186:0.004192:0.004192:0.001799:0.004192
a.  Used to evaluate  model performance:@0.399349:0.192329:0.674773:0.192329:0.674773:0.179303:0.399349:0.179303:0.007787:0.003320:0.004192:0.005630:0.010510:0.006486:0.008001:0.009010:0.004192:0.005186:0.008965:0.004192:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.008001:0.004192:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.008995:0.008001:0.005324:0.004788:0.008965:0.005324:0.013171:0.007787:0.008659:0.007068:0.008001
 :@0.072464:0.214347:0.076655:0.214347:0.076655:0.201321:0.072464:0.201321:0.004192
2.  Testing Set   :@0.101254:0.214347:0.215784:0.214347:0.215784:0.201321:0.101254:0.201321:0.008246:0.003320:0.004192:0.005370:0.006522:0.008001:0.006486:0.005186:0.003702:0.008659:0.009010:0.004192:0.008123:0.008001:0.005186:0.004192:0.011952:0.004192
b.  Used to train the model:@0.399349:0.214347:0.580553:0.214347:0.580553:0.201321:0.399349:0.201321:0.008995:0.003320:0.004192:0.004421:0.010510:0.006486:0.008001:0.009010:0.004192:0.005186:0.008965:0.004192:0.005186:0.005324:0.007787:0.003702:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702
 :@0.072464:0.236365:0.076655:0.236365:0.076655:0.223339:0.072464:0.223339:0.004192
3.  Overfitting   :@0.101254:0.236365:0.215784:0.236365:0.215784:0.223339:0.101254:0.223339:0.008246:0.003320:0.004192:0.005370:0.011535:0.007328:0.008001:0.005324:0.004788:0.003702:0.005186:0.005186:0.003702:0.008659:0.009010:0.004192:0.012599:0.004192
c.  Model generalizes poorly to new data:@0.399349:0.236365:0.675232:0.236365:0.675232:0.223339:0.399349:0.223339:0.007068:0.003320:0.004192:0.006349:0.013738:0.008965:0.009010:0.008001:0.003702:0.004192:0.009010:0.008001:0.008659:0.008001:0.005324:0.007787:0.003702:0.003702:0.006915:0.008001:0.006486:0.004192:0.008995:0.008965:0.008965:0.005324:0.003702:0.007404:0.004192:0.005186:0.008965:0.004192:0.008659:0.008001:0.011060:0.004192:0.009010:0.007787:0.005186:0.007787
 :@0.072464:0.258383:0.076655:0.258383:0.076655:0.245357:0.072464:0.245357:0.004192
4.  Underfitting   :@0.101254:0.258383:0.215784:0.258383:0.215784:0.245357:0.101254:0.245357:0.008246:0.003320:0.004192:0.005370:0.010510:0.008659:0.009010:0.008001:0.005324:0.004788:0.003702:0.005186:0.005186:0.003702:0.008659:0.009010:0.004192:0.003283:0.004192
d.  Model fails to capture underlying patterns:@0.399349:0.258383:0.704038:0.258383:0.704038:0.245357:0.399349:0.245357:0.009010:0.003320:0.004192:0.004406:0.013738:0.008965:0.009010:0.008001:0.003702:0.004192:0.004788:0.007787:0.003702:0.003702:0.006486:0.004192:0.005186:0.008965:0.004192:0.007068:0.007787:0.008995:0.005186:0.008659:0.005324:0.008001:0.004192:0.008659:0.008659:0.009010:0.008001:0.005324:0.003702:0.007404:0.003702:0.008659:0.009010:0.004192:0.008995:0.007787:0.005186:0.005186:0.008001:0.005324:0.008659:0.006486
D.  State whether these statements are true or false.:@0.072464:0.290599:0.468743:0.290599:0.468743:0.276152:0.072464:0.276152:0.011163:0.004075:0.004650:0.005881:0.008718:0.006104:0.008826:0.006016:0.008978:0.004650:0.012783:0.009841:0.008978:0.006104:0.009841:0.008978:0.006256:0.004650:0.006104:0.009841:0.008978:0.007287:0.008978:0.004650:0.007287:0.006104:0.008826:0.006057:0.008978:0.014981:0.008978:0.009857:0.006104:0.007287:0.004650:0.008826:0.006129:0.008978:0.004650:0.006104:0.006256:0.009857:0.008978:0.004650:0.010094:0.006256:0.004650:0.005833:0.008826:0.004413:0.007287:0.008978:0.004075
 :@0.072464:0.312042:0.076655:0.312042:0.076655:0.299016:0.072464:0.299016:0.004192
1.  Accuracy is always the best metric to evaluate a classification model. :@0.101254:0.312042:0.585754:0.312042:0.585754:0.299016:0.101254:0.299016:0.008246:0.003320:0.004192:0.005370:0.009867:0.007068:0.007068:0.008659:0.005324:0.007787:0.007068:0.007404:0.004192:0.003702:0.006486:0.004192:0.007787:0.003702:0.011060:0.007787:0.007404:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.008995:0.008001:0.006486:0.005186:0.004192:0.013171:0.008001:0.005186:0.005324:0.003702:0.007068:0.004192:0.005186:0.008965:0.004192:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.008001:0.004192:0.007787:0.004192:0.007068:0.003702:0.007787:0.006486:0.006486:0.003702:0.004788:0.003702:0.007068:0.007787:0.005186:0.003702:0.008965:0.008659:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.003320:0.004192
……….……:@0.853667:0.312042:0.913053:0.312042:0.913053:0.299016:0.853667:0.299016:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213
 :@0.072464:0.334060:0.076655:0.334060:0.076655:0.321034:0.072464:0.321034:0.004192
2.  A high recall means the model correctly identifies most of the actual positive cases. :@0.101254:0.334060:0.685726:0.334060:0.685726:0.321034:0.101254:0.321034:0.008246:0.003320:0.004192:0.005370:0.009867:0.004192:0.008659:0.003702:0.009010:0.008659:0.004192:0.005324:0.008001:0.007068:0.007787:0.003702:0.003702:0.004192:0.013171:0.008001:0.007787:0.008659:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.003702:0.007404:0.004192:0.003702:0.009010:0.008001:0.008659:0.005186:0.003702:0.004788:0.003702:0.008001:0.006486:0.004192:0.013171:0.008965:0.006486:0.005186:0.004192:0.008965:0.004788:0.004192:0.005186:0.008659:0.008001:0.004192:0.007787:0.007068:0.005186:0.008659:0.007787:0.003702:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.004192:0.007068:0.007787:0.006486:0.008001:0.006486:0.003320:0.004192
……….……:@0.853667:0.334060:0.913053:0.334060:0.913053:0.321034:0.853667:0.321034:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213
 :@0.072464:0.356078:0.076655:0.356078:0.076655:0.343052:0.072464:0.343052:0.004192
3.  The training set is used to evaluate the model’s performance on unseen data. :@0.101254:0.356078:0.646135:0.356078:0.646135:0.343052:0.101254:0.343052:0.008246:0.003320:0.004192:0.005370:0.008016:0.008659:0.008001:0.004192:0.005186:0.005324:0.007787:0.003702:0.008659:0.003702:0.008659:0.009010:0.004192:0.006486:0.008001:0.005186:0.004192:0.003702:0.006486:0.004192:0.008659:0.006486:0.008001:0.009010:0.004192:0.005186:0.008965:0.004192:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.008001:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.003503:0.006486:0.004192:0.008995:0.008001:0.005324:0.004788:0.008965:0.005324:0.013171:0.007787:0.008659:0.007068:0.008001:0.004192:0.008965:0.008659:0.004192:0.008659:0.008659:0.006486:0.008001:0.008001:0.008659:0.004192:0.009010:0.007787:0.005186:0.007787:0.003320:0.004192
……….……:@0.853667:0.356078:0.913053:0.356078:0.913053:0.343052:0.853667:0.343052:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213
 :@0.072464:0.378096:0.076655:0.378096:0.076655:0.365070:0.072464:0.365070:0.004192
4.  The F1 Score is the harmonic mean of Precision and Recall. :@0.101254:0.378096:0.521280:0.378096:0.521280:0.365070:0.101254:0.365070:0.008246:0.003320:0.004192:0.005370:0.008016:0.008659:0.008001:0.004192:0.007465:0.008246:0.004192:0.008123:0.007068:0.008965:0.005324:0.008001:0.004192:0.003702:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.008659:0.007787:0.005324:0.013171:0.008965:0.008659:0.003702:0.007068:0.004192:0.013171:0.008001:0.007787:0.008659:0.004192:0.008965:0.004788:0.004192:0.008567:0.005324:0.008001:0.007068:0.003702:0.006486:0.003702:0.008965:0.008659:0.004192:0.007787:0.008659:0.009010:0.004192:0.008680:0.008001:0.007068:0.007787:0.003702:0.003702:0.003320:0.004192
……….……:@0.853667:0.378096:0.913053:0.378096:0.913053:0.365070:0.853667:0.365070:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213
 :@0.072464:0.400114:0.076655:0.400114:0.076655:0.387088:0.072464:0.387088:0.004192
5.  Overfitting occurs when a model performs well on the training data but poorly on new data. :@0.101254:0.400114:0.745862:0.400114:0.745862:0.387088:0.101254:0.387088:0.008246:0.003320:0.004192:0.005370:0.011535:0.007328:0.008001:0.005324:0.004788:0.003702:0.005186:0.005186:0.003702:0.008659:0.009010:0.004192:0.008965:0.007068:0.007068:0.008659:0.005324:0.006486:0.004192:0.011060:0.008659:0.008001:0.008659:0.004192:0.007787:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.008995:0.008001:0.005324:0.004788:0.008965:0.005324:0.013171:0.006486:0.004192:0.011060:0.008001:0.003702:0.003702:0.004192:0.008965:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.005186:0.005324:0.007787:0.003702:0.008659:0.003702:0.008659:0.009010:0.004192:0.009010:0.007787:0.005186:0.007787:0.004192:0.008995:0.008659:0.005186:0.004192:0.008995:0.008965:0.008965:0.005324:0.003702:0.007404:0.004192:0.008965:0.008659:0.004192:0.008659:0.008001:0.011060:0.004192:0.009010:0.007787:0.005186:0.007787:0.003320:0.004192
……….……:@0.853667:0.400114:0.913053:0.400114:0.913053:0.387088:0.853667:0.387088:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213
SECTION B :@0.337805:0.436965:0.432766:0.436965:0.432766:0.421817:0.337805:0.421817:0.009937:0.009424:0.011053:0.010380:0.005615:0.013427:0.013994:0.004889:0.011354:0.004889
(Subjective Type Questions):@0.432763:0.436781:0.647708:0.436781:0.647708:0.421698:0.432763:0.421698:0.005349:0.009406:0.010026:0.010415:0.004287:0.009264:0.008184:0.006005:0.004287:0.008485:0.009264:0.004853:0.008300:0.008573:0.010415:0.009264:0.004853:0.013356:0.010026:0.009264:0.007510:0.006005:0.004287:0.010380:0.010026:0.007510:0.005349
A.  Short answer type questions.:@0.072464:0.460297:0.320117:0.460297:0.320117:0.445850:0.072464:0.445850:0.011345:0.004075:0.004650:0.005698:0.009198:0.009841:0.010094:0.006757:0.006104:0.004650:0.008826:0.009857:0.007287:0.012720:0.008978:0.006256:0.004650:0.006104:0.008589:0.010196:0.008978:0.004650:0.010196:0.009857:0.008978:0.007287:0.006104:0.004413:0.010094:0.009857:0.007287:0.004075
 :@0.072464:0.481740:0.076655:0.481740:0.076655:0.468715:0.072464:0.468715:0.004192
1.  What does classification refer to in machine  learning?:@0.101254:0.481740:0.486073:0.481740:0.486073:0.468715:0.101254:0.468715:0.008246:0.003320:0.004192:0.005370:0.014288:0.008659:0.007787:0.005186:0.004192:0.009010:0.008965:0.008001:0.006486:0.004192:0.007068:0.003702:0.007787:0.006486:0.006486:0.003702:0.004788:0.003702:0.007068:0.007787:0.005186:0.003702:0.008965:0.008659:0.004192:0.005324:0.008001:0.004788:0.008001:0.005324:0.004192:0.005186:0.008965:0.004192:0.003702:0.008659:0.004192:0.013171:0.007787:0.007068:0.008659:0.003702:0.008659:0.008001:0.004192:0.004192:0.003702:0.008001:0.007787:0.005324:0.008659:0.003702:0.008659:0.009010:0.006853
 :@0.072464:0.503759:0.076655:0.503759:0.076655:0.490733:0.072464:0.490733:0.004192
2.  Can we use  Accuracy all the time?:@0.101254:0.503759:0.354037:0.503759:0.354037:0.490733:0.101254:0.490733:0.008246:0.003320:0.004192:0.005370:0.009469:0.007787:0.008659:0.004192:0.011060:0.008001:0.004192:0.008659:0.006486:0.008001:0.004192:0.004192:0.009867:0.007068:0.007068:0.008659:0.005324:0.007787:0.007068:0.007404:0.004192:0.007787:0.003702:0.003702:0.004192:0.005186:0.008659:0.008001:0.004192:0.005186:0.003702:0.013171:0.008001:0.006853
 :@0.072464:0.525777:0.076655:0.525777:0.076655:0.512751:0.072464:0.512751:0.004192
3.  What is Precision  in model  evaluation, and why is it important in spam detection?:@0.101254:0.525777:0.678200:0.525777:0.678200:0.512751:0.101254:0.512751:0.008246:0.003320:0.004192:0.005370:0.014288:0.008659:0.007787:0.005186:0.004192:0.003702:0.006486:0.004192:0.008567:0.005324:0.008001:0.007068:0.003702:0.006486:0.003702:0.008965:0.008659:0.004192:0.004192:0.003702:0.008659:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.004192:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.003702:0.008965:0.008659:0.003320:0.004192:0.007787:0.008659:0.009010:0.004192:0.011060:0.008659:0.007404:0.004192:0.003702:0.006486:0.004192:0.003702:0.005186:0.004192:0.003702:0.013171:0.008995:0.008965:0.005324:0.005186:0.007787:0.008659:0.005186:0.004192:0.003702:0.008659:0.004192:0.006486:0.008995:0.007787:0.013171:0.004192:0.009010:0.008001:0.005186:0.008001:0.007068:0.005186:0.003702:0.008965:0.008659:0.006853
 :@0.072464:0.547795:0.076655:0.547795:0.076655:0.534769:0.072464:0.534769:0.004192
4.  How does the F1 Score help in evaluating a classification model?:@0.101254:0.547795:0.556795:0.547795:0.556795:0.534769:0.101254:0.534769:0.008246:0.003320:0.004192:0.005370:0.010862:0.008965:0.011060:0.004192:0.009010:0.008965:0.008001:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.007465:0.008246:0.004192:0.008123:0.007068:0.008965:0.005324:0.008001:0.004192:0.008659:0.008001:0.003702:0.008995:0.004192:0.003702:0.008659:0.004192:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.003702:0.008659:0.009010:0.004192:0.007787:0.004192:0.007068:0.003702:0.007787:0.006486:0.006486:0.003702:0.004788:0.003702:0.007068:0.007787:0.005186:0.003702:0.008965:0.008659:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.006853
 :@0.072464:0.569813:0.076655:0.569813:0.076655:0.556787:0.072464:0.556787:0.004192
5.  Why is it important  to split  the dataset into  training  and  testing  sets?:@0.101254:0.569813:0.610981:0.569813:0.610981:0.556787:0.101254:0.556787:0.008246:0.003320:0.004192:0.005370:0.014288:0.008659:0.007404:0.004192:0.003702:0.006486:0.004192:0.003702:0.005186:0.004192:0.003702:0.013171:0.008995:0.008965:0.005324:0.005186:0.007787:0.008659:0.005186:0.004192:0.004192:0.005186:0.008965:0.004192:0.006486:0.008995:0.003702:0.003702:0.005186:0.004192:0.004192:0.005186:0.008659:0.008001:0.004192:0.009010:0.007787:0.005186:0.007787:0.006486:0.008001:0.005186:0.004192:0.003702:0.008659:0.005186:0.008965:0.004192:0.004192:0.005186:0.005324:0.007787:0.003702:0.008659:0.003702:0.008659:0.009010:0.004192:0.004192:0.007787:0.008659:0.009010:0.004192:0.004192:0.005186:0.008001:0.006486:0.005186:0.003702:0.008659:0.009010:0.004192:0.004192:0.006486:0.008001:0.005186:0.006486:0.006853
B.  Long answer type questions.:@0.072464:0.597479:0.316534:0.597479:0.316534:0.583032:0.072464:0.583032:0.010213:0.004075:0.004650:0.006831:0.008268:0.010094:0.009857:0.010196:0.004650:0.008826:0.009857:0.007287:0.012715:0.008978:0.006256:0.004650:0.006104:0.008589:0.010196:0.008978:0.004650:0.010196:0.009857:0.008978:0.007287:0.006104:0.004413:0.010094:0.009857:0.007287:0.004075
 :@0.072464:0.623656:0.076655:0.623656:0.076655:0.610630:0.072464:0.610630:0.004192
1.  Explain  Accuracy, Precision, Recall, and F1 Score with examples. When  should  each metric be used?:@0.101254:0.623656:0.799651:0.623656:0.799651:0.610630:0.101254:0.610630:0.008246:0.003320:0.004192:0.005370:0.007741:0.007022:0.008995:0.003702:0.007787:0.003702:0.008659:0.004192:0.004192:0.009867:0.007068:0.007068:0.008659:0.005324:0.007787:0.007068:0.007404:0.003320:0.004192:0.008567:0.005324:0.008001:0.007068:0.003702:0.006486:0.003702:0.008965:0.008659:0.003320:0.004192:0.008676:0.008001:0.007068:0.007787:0.003702:0.003702:0.003320:0.004192:0.007787:0.008659:0.009010:0.004192:0.007465:0.008246:0.004192:0.008123:0.007068:0.008965:0.005324:0.008001:0.004192:0.011060:0.003702:0.005186:0.008659:0.004192:0.008001:0.007022:0.007787:0.013171:0.008995:0.003702:0.008001:0.006486:0.003320:0.004192:0.014288:0.008659:0.008001:0.008659:0.004192:0.004192:0.006486:0.008659:0.008965:0.008659:0.003702:0.009010:0.004192:0.004192:0.008001:0.007787:0.007068:0.008659:0.004192:0.013171:0.008001:0.005186:0.005324:0.003702:0.007068:0.004192:0.008995:0.008001:0.004192:0.008659:0.006486:0.008001:0.009010:0.006853
 :@0.072464:0.646909:0.076655:0.646909:0.076655:0.633883:0.072464:0.633883:0.004192
2.  Which metric is more important—Recall or Precision? Explain in detail.:@0.101254:0.646909:0.594096:0.646909:0.594096:0.633883:0.101254:0.633883:0.008246:0.003320:0.004192:0.005370:0.014288:0.008659:0.003702:0.007068:0.008659:0.004192:0.013171:0.008001:0.005186:0.005324:0.003702:0.007068:0.004192:0.003702:0.006486:0.004192:0.013171:0.008965:0.005324:0.008001:0.004192:0.003702:0.013171:0.008995:0.008965:0.005324:0.005186:0.007787:0.008659:0.005186:0.015298:0.008694:0.008001:0.007068:0.007787:0.003702:0.003702:0.004192:0.008965:0.005324:0.004192:0.008567:0.005324:0.008001:0.007068:0.003702:0.006486:0.003702:0.008965:0.008659:0.006853:0.004192:0.007741:0.007022:0.008995:0.003702:0.007787:0.003702:0.008659:0.004192:0.003702:0.008659:0.004192:0.009010:0.008001:0.005186:0.007787:0.003702:0.003702:0.003320
 :@0.072464:0.670161:0.076655:0.670161:0.076655:0.657136:0.072464:0.657136:0.004192
3.  Describe  the concept of overfitting and underfitting. How can they  impact model evaluation, and how can we prevent :@0.101254:0.670161:0.917245:0.670161:0.917245:0.657136:0.101254:0.657136:0.008246:0.003320:0.004192:0.005370:0.010724:0.008001:0.006486:0.007068:0.005324:0.003702:0.008995:0.008001:0.003722:0.003722:0.005186:0.008659:0.008001:0.003717:0.007068:0.008965:0.008659:0.007068:0.008001:0.008995:0.005186:0.003713:0.008965:0.004788:0.003720:0.008965:0.007328:0.008001:0.005324:0.004788:0.003702:0.005186:0.005186:0.003702:0.008659:0.009010:0.003713:0.007787:0.008659:0.009010:0.003714:0.008659:0.008659:0.009010:0.008001:0.005324:0.004788:0.003702:0.005186:0.005186:0.003702:0.008659:0.009010:0.003320:0.003707:0.010862:0.008965:0.011060:0.003714:0.007068:0.007787:0.008659:0.003716:0.005186:0.008659:0.008001:0.007404:0.003716:0.003722:0.003702:0.013171:0.008995:0.007787:0.007068:0.005186:0.003720:0.013171:0.008965:0.009010:0.008001:0.003702:0.003725:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.003702:0.008965:0.008659:0.003320:0.003713:0.007787:0.008659:0.009010:0.003714:0.008659:0.008965:0.011060:0.003714:0.007068:0.007787:0.008659:0.003716:0.011060:0.008001:0.003716:0.008995:0.005324:0.008001:0.007328:0.008001:0.008659:0.005186:0.004192
them?:@0.122383:0.689915:0.164254:0.689915:0.164254:0.676889:0.122383:0.676889:0.005186:0.008659:0.008001:0.013171:0.006853
 :@0.072464:0.713167:0.076655:0.713167:0.076655:0.700141:0.072464:0.700141:0.004192
4.  Why is model evaluation important in machine learning? Discuss different techniques used to evaluate classification  :@0.101254:0.713167:0.917234:0.713167:0.917234:0.700141:0.101254:0.700141:0.008246:0.003320:0.004192:0.005370:0.014288:0.008659:0.007404:0.005076:0.003702:0.006486:0.005085:0.013171:0.008965:0.009010:0.008001:0.003702:0.005077:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.003702:0.008965:0.008659:0.005076:0.003702:0.013171:0.008995:0.008965:0.005324:0.005186:0.007787:0.008659:0.005186:0.005070:0.003702:0.008659:0.005079:0.013171:0.007787:0.007068:0.008659:0.003702:0.008659:0.008001:0.005077:0.003702:0.008001:0.007787:0.005324:0.008659:0.003702:0.008659:0.009010:0.006853:0.005073:0.010724:0.003702:0.006486:0.007068:0.008659:0.006486:0.006486:0.005094:0.009010:0.003702:0.004788:0.004788:0.008001:0.005324:0.008001:0.008659:0.005186:0.005068:0.005186:0.008001:0.007068:0.008659:0.008659:0.003702:0.009010:0.008659:0.008001:0.006486:0.005074:0.008659:0.006486:0.008001:0.009010:0.005077:0.005186:0.008965:0.005076:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.008001:0.005068:0.007068:0.003702:0.007787:0.006486:0.006486:0.003702:0.004788:0.003702:0.007068:0.007787:0.005186:0.003702:0.008965:0.008659:0.000000:0.004192
models.:@0.122383:0.732920:0.175039:0.732920:0.175039:0.719894:0.122383:0.719894:0.013171:0.008965:0.009010:0.008001:0.003702:0.006486:0.003320
C.  Competency-based/Application-based questions. :@0.072464:0.764086:0.480792:0.764086:0.480792:0.749639:0.072464:0.749639:0.010500:0.004075:0.004650:0.006543:0.010500:0.010094:0.014981:0.010196:0.008978:0.006008:0.008978:0.009857:0.007947:0.008589:0.006797:0.010008:0.008826:0.007287:0.008978:0.010196:0.007000:0.011345:0.010196:0.010196:0.004413:0.004413:0.007947:0.008826:0.006104:0.004413:0.010094:0.009857:0.006797:0.010001:0.008826:0.007287:0.008978:0.010196:0.004650:0.010196:0.009857:0.008978:0.007287:0.006104:0.004413:0.010094:0.009857:0.007287:0.004075:0.004650
 :@0.072464:0.786404:0.076655:0.786404:0.076655:0.773378:0.072464:0.773378:0.004192
1.  A credit scoring model is used to predict whether an applicant is likely to default on a loan (1) or not (0). Out of 1000 :@0.101254:0.786404:0.917233:0.786404:0.917233:0.773378:0.101254:0.773378:0.008246:0.003320:0.004192:0.005370:0.009867:0.004414:0.007068:0.005324:0.008001:0.009010:0.003702:0.005186:0.004404:0.006486:0.007068:0.008965:0.005324:0.003702:0.008659:0.009010:0.004409:0.013171:0.008965:0.009010:0.008001:0.003702:0.004416:0.003702:0.006486:0.004421:0.008659:0.006486:0.008001:0.009010:0.004413:0.005186:0.008965:0.004410:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.004403:0.011060:0.008659:0.008001:0.005186:0.008659:0.008001:0.005324:0.004395:0.007787:0.008659:0.004409:0.007787:0.008995:0.008995:0.003702:0.003702:0.007068:0.007787:0.008659:0.005186:0.004407:0.003702:0.006486:0.004421:0.003702:0.003702:0.007603:0.008001:0.003702:0.007404:0.004420:0.005186:0.008965:0.004410:0.009010:0.008001:0.004788:0.007787:0.008659:0.003702:0.005186:0.004406:0.008965:0.008659:0.004410:0.007787:0.004409:0.003702:0.008965:0.007787:0.008659:0.004410:0.004620:0.008246:0.004620:0.004407:0.008965:0.005324:0.004406:0.008659:0.008965:0.005186:0.004409:0.004620:0.008246:0.004620:0.003320:0.004403:0.011535:0.008659:0.005186:0.004409:0.008965:0.004788:0.004412:0.008246:0.008246:0.008246:0.008246:0.004192
loan applicants: :@0.122383:0.804923:0.231564:0.804923:0.231564:0.791897:0.122383:0.791897:0.003702:0.008965:0.007787:0.008659:0.004192:0.007787:0.008995:0.008995:0.003702:0.003702:0.007068:0.007787:0.008659:0.005186:0.006486:0.003320:0.004192
[CBSE Handbook]:@0.794680:0.804923:0.913056:0.804923:0.913056:0.791897:0.794680:0.791897:0.004620:0.009469:0.008766:0.008123:0.007741:0.004192:0.010862:0.007787:0.008659:0.009010:0.008995:0.008965:0.008965:0.007603:0.004620
 :@0.072464:0.826066:0.076655:0.826066:0.076655:0.813040:0.072464:0.813040:0.004192
  True Positives(TP): 90 applicants were correctly predicted to default on the loan.:@0.109502:0.826066:0.656911:0.826066:0.656911:0.813040:0.109502:0.813040:0.004192:0.008688:0.006685:0.005324:0.008659:0.008001:0.004192:0.007992:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.006486:0.004620:0.008016:0.008567:0.004620:0.003320:0.004192:0.008246:0.008246:0.004192:0.007787:0.008995:0.008995:0.003702:0.003702:0.007068:0.007787:0.008659:0.005186:0.006486:0.004192:0.011060:0.008001:0.005324:0.008001:0.004192:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.003702:0.007404:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.008001:0.009010:0.004192:0.005186:0.008965:0.004192:0.009010:0.008001:0.004788:0.007787:0.008659:0.003702:0.005186:0.004192:0.008965:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.003702:0.008965:0.007787:0.008659:0.003320
 :@0.072464:0.847209:0.076655:0.847209:0.076655:0.834183:0.072464:0.834183:0.004192
  False Positives(FP): 40 applicants were incorrectly predicted to default on the loan.:@0.109502:0.847209:0.672986:0.847209:0.672986:0.834183:0.109502:0.834183:0.004192:0.008688:0.006947:0.007787:0.003702:0.006486:0.008001:0.004192:0.008002:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.006486:0.004620:0.007465:0.008567:0.004620:0.003320:0.004192:0.008246:0.008246:0.004192:0.007787:0.008995:0.008995:0.003702:0.003702:0.007068:0.007787:0.008659:0.005186:0.006486:0.004192:0.011060:0.008001:0.005324:0.008001:0.004192:0.003702:0.008659:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.003702:0.007404:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.008001:0.009010:0.004192:0.005186:0.008965:0.004192:0.009010:0.008001:0.004788:0.007787:0.008659:0.003702:0.005186:0.004192:0.008965:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.003702:0.008965:0.007787:0.008659:0.003320
 :@0.072464:0.868352:0.076655:0.868352:0.076655:0.855326:0.072464:0.855326:0.004192
  True Negatives(TN): 820 applicants were correctly predicted not to default on the loan.:@0.109502:0.868352:0.704130:0.868352:0.704130:0.855326:0.109502:0.855326:0.004192:0.008688:0.006685:0.005324:0.008659:0.008001:0.004192:0.011443:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.006486:0.004620:0.008016:0.011443:0.004620:0.003320:0.004192:0.008246:0.008246:0.008246:0.004192:0.007787:0.008995:0.008995:0.003702:0.003702:0.007068:0.007787:0.008659:0.005186:0.006486:0.004192:0.011060:0.008001:0.005324:0.008001:0.004192:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.003702:0.007404:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.008001:0.009010:0.004192:0.008659:0.008965:0.005186:0.004192:0.005186:0.008965:0.004192:0.009010:0.008001:0.004788:0.007787:0.008659:0.003702:0.005186:0.004192:0.008965:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.003702:0.008965:0.007787:0.008659:0.003320
 :@0.072464:0.889495:0.076655:0.889495:0.076655:0.876470:0.072464:0.876470:0.004192
  False Negatives (FN): 50 applicants were incorrectly predicted not to default on the loan.:@0.109502:0.889495:0.716140:0.889495:0.716140:0.876470:0.109502:0.876470:0.004192:0.008688:0.006947:0.007787:0.003702:0.006486:0.008001:0.004192:0.011443:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.006486:0.004192:0.004620:0.007465:0.011443:0.004620:0.003320:0.004192:0.008246:0.008246:0.004192:0.007787:0.008995:0.008995:0.003702:0.003702:0.007068:0.007787:0.008659:0.005186:0.006486:0.004192:0.011060:0.008001:0.005324:0.008001:0.004192:0.003702:0.008659:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.003702:0.007404:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.008001:0.009010:0.004192:0.008659:0.008965:0.005186:0.004192:0.005186:0.008965:0.004192:0.009010:0.008001:0.004788:0.007787:0.008659:0.003702:0.005186:0.004192:0.008965:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.003702:0.008965:0.007787:0.008659:0.003320
 :@0.072464:0.912388:0.076655:0.912388:0.076655:0.899362:0.072464:0.899362:0.004192
  Calculate metrics such as accuracy, precision, recall, and F1-score.:@0.109502:0.912388:0.562410:0.912388:0.562410:0.899362:0.109502:0.899362:0.004192:0.008688:0.009469:0.007787:0.003702:0.007068:0.008659:0.003702:0.007787:0.005186:0.008001:0.004192:0.013171:0.008001:0.005186:0.005324:0.003702:0.007068:0.006486:0.004192:0.006486:0.008659:0.007068:0.008659:0.004192:0.007787:0.006486:0.004192:0.007787:0.007068:0.007068:0.008659:0.005324:0.007787:0.007068:0.007404:0.003320:0.004192:0.008995:0.005324:0.008001:0.007068:0.003702:0.006486:0.003702:0.008965:0.008659:0.003320:0.004192:0.005324:0.008001:0.007068:0.007787:0.003702:0.003702:0.003320:0.004192:0.007787:0.008659:0.009010:0.004192:0.007465:0.008246:0.006119:0.006486:0.007068:0.008965:0.005324:0.008001:0.003320
#Critical Thinking:@0.792520:0.763931:0.909376:0.763931:0.909376:0.749275:0.792520:0.749275:0.009362:0.009145:0.005913:0.003667:0.005116:0.003667:0.006899:0.008058:0.003667:0.003768:0.008014:0.008899:0.003667:0.008899:0.007609:0.003667:0.008899:0.007942
21 Century :@0.727352:0.757564:0.783888:0.757564:0.783888:0.747793:0.727352:0.747793:0.005527:0.005527:0.007367:0.006097:0.005420:0.005932:0.003411:0.005932:0.003942:0.004870:0.002512
st :@0.738401:0.754495:0.745773:0.754495:0.745773:0.747656:0.738401:0.747656:0.003226:0.002387:0.001758
Skills:@0.759057:0.766453:0.781376:0.766453:0.781376:0.756682:0.759057:0.756682:0.005304:0.005072:0.002444:0.002444:0.002444:0.004609