T:@0.180169:0.948923:0.188607:0.948923:0.188607:0.935211:0.180169:0.935211:0.008438
ouchpad Artificial Intelligence (Ver. 3.0):@0.187317:0.948923:0.463822:0.948083:0.463822:0.934372:0.187317:0.935211:0.009436:0.009114:0.007440:0.009114:0.009469:0.008196:0.009485:0.004412:0.010386:0.005604:0.005459:0.003897:0.004469:0.004469:0.007440:0.003897:0.008196:0.003897:0.004412:0.004283:0.009114:0.005459:0.008422:0.003897:0.003897:0.003897:0.009485:0.008422:0.009114:0.007440:0.008422:0.004412:0.004863:0.008952:0.008422:0.005604:0.003494:0.004412:0.008680:0.003494:0.008665:-0.373357
-X:@0.463821:0.948260:0.480037:0.948926:0.480037:0.935215:0.463821:0.934548:0.006715:-0.386033
390:@0.118726:0.949910:0.144765:0.949910:0.144765:0.936198:0.118726:0.936198:0.008680:0.008680:0.008680
Factor:@0.162193:0.079220:0.206596:0.079220:0.206596:0.066137:0.162193:0.066137:0.007499:0.008230:0.007343:0.005896:0.009347:0.006089
Smart-bot:@0.387441:0.079220:0.461018:0.079220:0.461018:0.066137:0.387441:0.066137:0.008582:0.014013:0.008230:0.006523:0.005266:0.006180:0.009485:0.009347:0.005951
Script-bot:@0.721943:0.079220:0.794072:0.079220:0.794072:0.066137:0.721943:0.066137:0.008582:0.007343:0.006141:0.004345:0.009485:0.005270:0.006180:0.009485:0.009347:0.005951
Adaptability:@0.132271:0.112794:0.213594:0.112794:0.213594:0.099768:0.132271:0.099768:0.009867:0.009010:0.007787:0.008995:0.005186:0.007787:0.008995:0.003702:0.003702:0.003702:0.005186:0.007404
Can  adapt  to  new  scenarios  and  learn  from  Fixed  functionality;  cannot  adapt  to  new :@0.253913:0.103534:0.908281:0.103534:0.908281:0.090508:0.253913:0.090508:0.009469:0.007787:0.008659:0.004192:0.007305:0.007787:0.009010:0.007787:0.008995:0.005186:0.004192:0.007300:0.005186:0.008965:0.004192:0.007303:0.008659:0.008001:0.011060:0.004192:0.007299:0.006486:0.007068:0.008001:0.008659:0.007787:0.005324:0.003702:0.008965:0.006486:0.004192:0.007311:0.007787:0.008659:0.009010:0.004192:0.007300:0.003702:0.008001:0.007787:0.005324:0.008659:0.004192:0.007299:0.004788:0.005324:0.008965:0.013171:0.004192:0.013187:0.007465:0.003702:0.007022:0.008001:0.009010:0.004192:0.004456:0.004788:0.008659:0.008659:0.007068:0.005186:0.003702:0.008965:0.008659:0.007787:0.003702:0.003702:0.005186:0.007404:0.003320:0.004192:0.004443:0.007068:0.007787:0.008659:0.008659:0.008965:0.005186:0.004192:0.004443:0.007787:0.009010:0.007787:0.008995:0.005186:0.004192:0.004446:0.005186:0.008965:0.004192:0.004450:0.008659:0.008001:0.011060:0.004192
interactions.:@0.253913:0.122053:0.335956:0.122053:0.335956:0.109027:0.253913:0.109027:0.003702:0.008659:0.005186:0.008001:0.005324:0.007787:0.007068:0.005186:0.003702:0.008965:0.008659:0.006486:0.003320
scenarios unless reprogrammed.:@0.611952:0.122053:0.830207:0.122053:0.830207:0.109027:0.611952:0.109027:0.006486:0.007068:0.008001:0.008659:0.007787:0.005324:0.003702:0.008965:0.006486:0.004192:0.008659:0.008659:0.003702:0.008001:0.006486:0.006486:0.004192:0.005324:0.008001:0.008995:0.005324:0.008965:0.009010:0.005324:0.007787:0.013171:0.013171:0.008001:0.009010:0.003320
Response :@0.132271:0.140961:0.200756:0.140961:0.200756:0.127936:0.132271:0.127936:0.008701:0.008001:0.006486:0.008995:0.008965:0.008659:0.006486:0.008001:0.004192
Complexity:@0.132271:0.159480:0.207888:0.159480:0.207888:0.146454:0.132271:0.146454:0.009469:0.008965:0.013171:0.008995:0.003702:0.008001:0.007022:0.003702:0.005186:0.007404
Handles  dynamic,  context-aware,  and  nuanced  Handles simple, straightforward tasks with :@0.253913:0.140961:0.908275:0.140961:0.908275:0.127936:0.253913:0.127936:0.010862:0.007787:0.008659:0.009010:0.003702:0.008001:0.006486:0.004192:0.006632:0.009010:0.007404:0.008659:0.007787:0.013171:0.003702:0.007068:0.003320:0.004192:0.006627:0.007068:0.008965:0.008659:0.005186:0.008001:0.007022:0.005186:0.006119:0.007787:0.011060:0.007787:0.005324:0.008001:0.003320:0.004192:0.006601:0.007787:0.008659:0.009010:0.004192:0.006625:0.008659:0.008659:0.007787:0.008659:0.007068:0.008001:0.009010:0.004192:0.013189:0.010862:0.007787:0.008659:0.009010:0.003702:0.008001:0.006486:0.006127:0.006486:0.003702:0.013171:0.008995:0.003702:0.008001:0.003320:0.006138:0.006486:0.005186:0.005324:0.007787:0.003702:0.009010:0.008659:0.005186:0.004788:0.008965:0.005324:0.011060:0.007787:0.005324:0.009010:0.006098:0.005186:0.007787:0.006486:0.007603:0.006486:0.006133:0.011060:0.003702:0.005186:0.008659:0.004192
conversations.:@0.253913:0.159480:0.349846:0.159480:0.349846:0.146454:0.253913:0.146454:0.007068:0.008965:0.008659:0.007328:0.008001:0.005324:0.006486:0.007787:0.005186:0.003702:0.008965:0.008659:0.006486:0.003320
limited scope.:@0.611952:0.159480:0.705452:0.159480:0.705452:0.146454:0.611952:0.146454:0.003702:0.003702:0.013171:0.003702:0.005186:0.008001:0.009010:0.004192:0.006486:0.007068:0.008965:0.008995:0.008001:0.003320
Learning Ability:@0.132271:0.188118:0.236518:0.188118:0.236518:0.175092:0.132271:0.175092:0.007205:0.008001:0.007787:0.005324:0.008659:0.003702:0.008659:0.009010:0.003343:0.009867:0.008995:0.003702:0.003702:0.003702:0.005186:0.007404
Can  improve  through  machine  learning  and  No  learning  capability;  requires  manual :@0.253913:0.178859:0.908271:0.178859:0.908271:0.165833:0.253913:0.165833:0.009469:0.007787:0.008659:0.004192:0.009301:0.003702:0.013171:0.008995:0.005324:0.008965:0.007328:0.008001:0.004192:0.009295:0.005186:0.008659:0.005324:0.008965:0.008659:0.009010:0.008659:0.004192:0.009289:0.013171:0.007787:0.007068:0.008659:0.003702:0.008659:0.008001:0.004192:0.009304:0.003702:0.008001:0.007787:0.005324:0.008659:0.003702:0.008659:0.009010:0.004192:0.009295:0.007787:0.008659:0.009010:0.004192:0.013199:0.011443:0.008965:0.004192:0.007054:0.003702:0.008001:0.007787:0.005324:0.008659:0.003702:0.008659:0.009010:0.004192:0.007046:0.007068:0.007787:0.008995:0.007787:0.008995:0.003702:0.003702:0.003702:0.005186:0.007404:0.003320:0.004192:0.007045:0.005324:0.008001:0.009010:0.008649:0.003702:0.005324:0.008001:0.006486:0.004192:0.007057:0.013171:0.007787:0.008659:0.008659:0.007787:0.003702:0.004192
feedback loops.:@0.253913:0.197378:0.359790:0.197378:0.359790:0.184352:0.253913:0.184352:0.004788:0.008001:0.008001:0.009010:0.008995:0.007787:0.007068:0.007603:0.004192:0.003702:0.008965:0.008965:0.008995:0.006486:0.003320
updates to change behaviour.:@0.611952:0.197378:0.812721:0.197378:0.812721:0.184352:0.611952:0.184352:0.008659:0.008995:0.009010:0.007787:0.005186:0.008001:0.006486:0.004192:0.005186:0.008965:0.004192:0.007068:0.008659:0.007787:0.008659:0.009010:0.008001:0.004192:0.008995:0.008001:0.008659:0.007787:0.007328:0.003702:0.008965:0.008659:0.005324:0.003320
Flexibility:@0.132271:0.218493:0.194854:0.218493:0.194854:0.205467:0.132271:0.205467:0.007465:0.003702:0.008001:0.007022:0.003702:0.008995:0.003702:0.003702:0.003702:0.005186:0.007404
Works across a variety of tasks and domains.:@0.253913:0.218493:0.553813:0.218493:0.553813:0.205467:0.253913:0.205467:0.013692:0.008965:0.005324:0.007603:0.006486:0.004192:0.007787:0.007068:0.005324:0.008965:0.006486:0.006486:0.004192:0.007787:0.004192:0.007328:0.007787:0.005324:0.003702:0.008001:0.005186:0.007404:0.004192:0.008965:0.004788:0.004192:0.005186:0.007787:0.006486:0.007603:0.006486:0.004192:0.007787:0.008659:0.009010:0.004192:0.009010:0.008965:0.013171:0.007787:0.003702:0.008659:0.006486:0.003320
Limited to specific tasks and workflows.:@0.611952:0.218493:0.877126:0.218493:0.877126:0.205467:0.611952:0.205467:0.007205:0.003702:0.013171:0.003702:0.005186:0.008001:0.009010:0.004192:0.005186:0.008965:0.004192:0.006486:0.008995:0.008001:0.007068:0.003702:0.004245:0.004245:0.007068:0.004192:0.005186:0.007787:0.006486:0.007603:0.006486:0.004192:0.007787:0.008659:0.009010:0.004192:0.011060:0.008965:0.005324:0.007603:0.004245:0.004245:0.008965:0.011060:0.006486:0.003320
Examples:@0.132271:0.246358:0.195176:0.246358:0.195176:0.233332:0.132271:0.233332:0.007741:0.007022:0.007787:0.013171:0.008995:0.003702:0.008001:0.006486
ChatGPT, customer service AI, personal assistants  Basic FAQ bots, data-entry bots, or script-:@0.253913:0.237099:0.904082:0.237099:0.904082:0.224073:0.253913:0.224073:0.009469:0.008659:0.007787:0.005186:0.010494:0.008567:0.008016:0.003320:0.006185:0.007068:0.008659:0.006486:0.005186:0.008965:0.013171:0.008001:0.005324:0.006191:0.006486:0.008001:0.005324:0.007328:0.003702:0.007068:0.008001:0.006193:0.009867:0.004069:0.003320:0.006191:0.008995:0.008001:0.005324:0.006486:0.008965:0.008659:0.007787:0.003702:0.006186:0.007787:0.006486:0.006486:0.003702:0.006486:0.005186:0.007787:0.008659:0.005186:0.006486:0.004192:0.013210:0.008766:0.007787:0.006486:0.003702:0.007068:0.006147:0.006574:0.009867:0.011535:0.006139:0.008995:0.008965:0.005186:0.006486:0.003320:0.006138:0.009010:0.007787:0.005186:0.007787:0.006119:0.008001:0.008659:0.005186:0.005324:0.007404:0.006118:0.008995:0.008965:0.005186:0.006486:0.003320:0.006138:0.008965:0.005324:0.006134:0.006486:0.007068:0.005324:0.003702:0.008995:0.005186:0.006119
like Siri or Alexa.:@0.253913:0.255617:0.364333:0.255617:0.364333:0.242591:0.253913:0.242591:0.003702:0.003702:0.007603:0.008001:0.004192:0.008123:0.003702:0.005324:0.003702:0.004192:0.008965:0.005324:0.004192:0.009867:0.003702:0.008001:0.007022:0.007787:0.003320
following automation.:@0.611952:0.255617:0.760081:0.255617:0.760081:0.242591:0.611952:0.242591:0.004788:0.008965:0.003702:0.003702:0.008965:0.011060:0.003702:0.008659:0.009010:0.004192:0.007787:0.008659:0.005186:0.008965:0.013171:0.007787:0.005186:0.003702:0.008965:0.008659:0.003320
Development :@0.132271:0.276460:0.227194:0.276460:0.227194:0.263434:0.132271:0.263434:0.010724:0.008001:0.007328:0.008001:0.003702:0.008965:0.008995:0.013171:0.008001:0.008659:0.005186:0.004192
Complexity:@0.132271:0.294978:0.207888:0.294978:0.207888:0.281952:0.132271:0.281952:0.009469:0.008965:0.013171:0.008995:0.003702:0.008001:0.007022:0.003702:0.005186:0.007404
High:  Requires  AI  training,  natural  language  Low  to  Medium:  Requires  scripting  and :@0.253913:0.276460:0.908274:0.276460:0.908274:0.263434:0.253913:0.263434:0.010862:0.003702:0.009010:0.008659:0.003320:0.004192:0.009454:0.008703:0.008001:0.009010:0.008659:0.003702:0.005324:0.008001:0.006486:0.004192:0.009459:0.009867:0.004069:0.004192:0.009460:0.005186:0.005324:0.007787:0.003702:0.008659:0.003702:0.008659:0.009010:0.003320:0.004192:0.009446:0.008659:0.007787:0.005186:0.008659:0.005324:0.007787:0.003702:0.004192:0.009445:0.003702:0.007787:0.008659:0.009010:0.008659:0.007787:0.009010:0.008001:0.004192:0.013186:0.007205:0.008965:0.011060:0.004192:0.005737:0.005186:0.008965:0.004192:0.005744:0.013738:0.008001:0.009010:0.003702:0.008659:0.013171:0.003320:0.004192:0.005750:0.008701:0.008001:0.009010:0.008659:0.003702:0.005324:0.008001:0.006486:0.004192:0.005746:0.006486:0.007068:0.005324:0.003702:0.008995:0.005186:0.003702:0.008659:0.009010:0.004192:0.005741:0.007787:0.008659:0.009010:0.004192
processing (NLP), and ongoing updates.:@0.253913:0.294978:0.523019:0.294978:0.523019:0.281952:0.253913:0.281952:0.008995:0.005324:0.008965:0.007068:0.008001:0.006486:0.006486:0.003702:0.008659:0.009010:0.004192:0.004620:0.011443:0.007205:0.008567:0.004620:0.003320:0.004192:0.007787:0.008659:0.009010:0.004192:0.008965:0.008659:0.009010:0.008965:0.003702:0.008659:0.009010:0.004192:0.008659:0.008995:0.009010:0.007787:0.005186:0.008001:0.006486:0.003320
workflow definitions only.:@0.611952:0.294978:0.784711:0.294978:0.784711:0.281952:0.611952:0.281952:0.011060:0.008965:0.005324:0.007603:0.004245:0.004245:0.008965:0.011060:0.004192:0.009010:0.008001:0.004245:0.004245:0.008659:0.003702:0.005186:0.003702:0.008965:0.008659:0.006486:0.004192:0.008965:0.008659:0.003702:0.007404:0.003320
 :@0.072850:0.318823:0.077042:0.318823:0.077042:0.305797:0.072850:0.305797:0.004192
3.  Following is the corpus::@0.101641:0.318823:0.281422:0.318823:0.281422:0.305797:0.101641:0.305797:0.008246:0.003320:0.004192:0.005370:0.007465:0.008965:0.003702:0.003702:0.008965:0.011060:0.003702:0.008659:0.009010:0.004192:0.003702:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.007068:0.008965:0.005324:0.008995:0.008659:0.006486:0.003320
Document 1::@0.122770:0.339251:0.214649:0.339251:0.214649:0.326168:0.122770:0.326168:0.011275:0.009347:0.007343:0.009255:0.014013:0.008276:0.009255:0.005951:0.004222:0.008796:0.004146
 This is my first experience of Text Mining.:@0.214662:0.339091:0.495349:0.339091:0.495349:0.326065:0.214662:0.326065:0.004192:0.008016:0.008659:0.003702:0.006486:0.004192:0.003702:0.006486:0.004192:0.013171:0.007404:0.004192:0.004788:0.003702:0.005324:0.006486:0.005186:0.004192:0.008001:0.007022:0.008995:0.008001:0.005324:0.003702:0.008001:0.008659:0.007068:0.008001:0.004192:0.008965:0.004788:0.004192:0.006519:0.008001:0.007022:0.005186:0.004192:0.013738:0.003702:0.008659:0.003702:0.008659:0.009010:0.003320
Document 2::@0.122770:0.359519:0.214649:0.359519:0.214649:0.346436:0.122770:0.346436:0.011275:0.009347:0.007343:0.009255:0.014013:0.008276:0.009255:0.005951:0.004222:0.008796:0.004146
 I have learnt new technique in this.:@0.214662:0.359359:0.452881:0.359359:0.452881:0.346333:0.214662:0.346333:0.004192:0.004069:0.004192:0.008659:0.007787:0.007328:0.008001:0.004192:0.003702:0.008001:0.007787:0.005324:0.008659:0.005186:0.004192:0.008659:0.008001:0.011060:0.004192:0.005186:0.008001:0.007068:0.008659:0.008659:0.003702:0.009010:0.008659:0.008001:0.004192:0.003702:0.008659:0.004192:0.005186:0.008659:0.003702:0.006486:0.003320
 :@0.072850:0.379628:0.077042:0.379628:0.077042:0.366602:0.072850:0.366602:0.004192
  Perform the following functions on the above corpus::@0.109888:0.379628:0.481609:0.379628:0.481609:0.366602:0.109888:0.366602:0.004192:0.008688:0.007999:0.008001:0.005324:0.004788:0.008965:0.005324:0.013171:0.004192:0.005186:0.008659:0.008001:0.004192:0.004788:0.008965:0.003702:0.003702:0.008965:0.011060:0.003702:0.008659:0.009010:0.004192:0.004788:0.008659:0.008659:0.007068:0.005186:0.003702:0.008965:0.008659:0.006486:0.004192:0.008965:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.007787:0.008995:0.008965:0.007328:0.008001:0.004192:0.007068:0.008965:0.005324:0.008995:0.008659:0.006486:0.003320
 :@0.072850:0.399896:0.077042:0.399896:0.077042:0.386870:0.072850:0.386870:0.004192
  a. Sentence Segmentation:@0.109888:0.399896:0.298403:0.399896:0.298403:0.386870:0.109888:0.386870:0.004192:0.008688:0.007787:0.003320:0.004192:0.008123:0.008001:0.008659:0.005186:0.008001:0.008659:0.007068:0.008001:0.004192:0.008123:0.008001:0.009010:0.013171:0.008001:0.008659:0.005186:0.007787:0.005186:0.003702:0.008965:0.008659
 :@0.072850:0.420164:0.077042:0.420164:0.077042:0.407138:0.072850:0.407138:0.004192
  b. Tokenization:@0.109888:0.420164:0.223931:0.420164:0.223931:0.407138:0.109888:0.407138:0.004192:0.008688:0.008995:0.003320:0.004192:0.006515:0.008965:0.007603:0.008001:0.008659:0.003702:0.006915:0.007787:0.005186:0.003702:0.008965:0.008659
 :@0.072850:0.440433:0.077042:0.440433:0.077042:0.427407:0.072850:0.427407:0.004192
  c. Stop words removal:@0.109888:0.440433:0.271627:0.440433:0.271627:0.427407:0.109888:0.427407:0.004192:0.008688:0.007068:0.003320:0.004192:0.007629:0.005186:0.008965:0.008995:0.004192:0.011060:0.008965:0.005324:0.009010:0.006486:0.004192:0.005324:0.008001:0.013171:0.008965:0.007328:0.007787:0.003702
 :@0.072850:0.462451:0.077042:0.462451:0.077042:0.449425:0.072850:0.449425:0.004192
  d. Lowercase conversion:@0.109888:0.462451:0.286531:0.462451:0.286531:0.449425:0.109888:0.449425:0.004192:0.008688:0.009010:0.003320:0.004192:0.007205:0.008965:0.011060:0.008001:0.005324:0.007068:0.007787:0.006486:0.008001:0.004192:0.007068:0.008965:0.008659:0.007328:0.008001:0.005324:0.006486:0.003702:0.008965:0.008659
 :@0.072850:0.481129:0.077072:0.481129:0.077072:0.468046:0.072850:0.468046:0.004222
Ans. :@0.084775:0.480969:0.117299:0.480969:0.117299:0.467943:0.084775:0.467943:0.009867:0.008659:0.006486:0.003320:0.004192
a. This is my first experience of Text Mining. :@0.122770:0.480969:0.418749:0.480969:0.418749:0.467943:0.122770:0.467943:0.007787:0.003320:0.004192:0.008016:0.008659:0.003702:0.006486:0.004192:0.003702:0.006486:0.004192:0.013171:0.007404:0.004192:0.004788:0.003702:0.005324:0.006486:0.005186:0.004192:0.008001:0.007022:0.008995:0.008001:0.005324:0.003702:0.008001:0.008659:0.007068:0.008001:0.004192:0.008965:0.004788:0.004192:0.006512:0.008001:0.007022:0.005186:0.004192:0.013738:0.003702:0.008659:0.003702:0.008659:0.009010:0.003320:0.004192
    I have learnt new techniques in this.:@0.122770:0.499488:0.379713:0.499488:0.379713:0.486462:0.122770:0.486462:0.004192:0.003855:0.004192:0.004192:0.004069:0.004192:0.008659:0.007787:0.007328:0.008001:0.004192:0.003702:0.008001:0.007787:0.005324:0.008659:0.005186:0.004192:0.008659:0.008001:0.011060:0.004192:0.005186:0.008001:0.007068:0.008659:0.008659:0.003702:0.009010:0.008659:0.008001:0.006486:0.004192:0.003702:0.008659:0.004192:0.005186:0.008659:0.003702:0.006486:0.003320
 :@0.072850:0.521506:0.077042:0.521506:0.077042:0.508480:0.072850:0.508480:0.004192
  b.  :@0.109888:0.521506:0.143465:0.521506:0.143465:0.508480:0.109888:0.508480:0.004192:0.008688:0.008995:0.003320:0.004192:0.004192
This:@0.185607:0.525668:0.212471:0.525668:0.212471:0.512642:0.185607:0.512642:0.008016:0.008659:0.003702:0.006486
is :@0.303493:0.525668:0.317873:0.525668:0.317873:0.512642:0.303493:0.512642:0.003702:0.006486:0.004192
my:@0.407855:0.525668:0.428431:0.525668:0.428431:0.512642:0.407855:0.512642:0.013171:0.007404
first:@0.514956:0.525668:0.540442:0.525668:0.540442:0.512642:0.514956:0.512642:0.004788:0.003702:0.005324:0.006486:0.005186
experience:@0.600869:0.525668:0.673641:0.525668:0.673641:0.512642:0.600869:0.512642:0.008001:0.007022:0.008995:0.008001:0.005324:0.003702:0.008001:0.008659:0.007068:0.008001
of:@0.739927:0.525668:0.753680:0.525668:0.753680:0.512642:0.739927:0.512642:0.008965:0.004788
Text:@0.842989:0.525668:0.869719:0.525668:0.869719:0.512642:0.842989:0.512642:0.006521:0.008001:0.007022:0.005186
Mining:@0.175312:0.544926:0.222781:0.544926:0.222781:0.531900:0.175312:0.531900:0.013738:0.003702:0.008659:0.003702:0.008659:0.009010
.:@0.306935:0.544926:0.310255:0.544926:0.310255:0.531900:0.306935:0.531900:0.003320
I:@0.416116:0.544926:0.420186:0.544926:0.420186:0.531900:0.416116:0.531900:0.004069
have:@0.511820:0.544926:0.543594:0.544926:0.543594:0.531900:0.511820:0.531900:0.008659:0.007787:0.007328:0.008001
learnt:@0.617926:0.544926:0.656584:0.544926:0.656584:0.531900:0.617926:0.531900:0.003702:0.008001:0.007787:0.005324:0.008659:0.005186
new:@0.732951:0.544926:0.760671:0.544926:0.760671:0.531900:0.732951:0.531900:0.008659:0.008001:0.011060
techniques:@0.819644:0.544926:0.893074:0.544926:0.893074:0.531900:0.819644:0.531900:0.005186:0.008001:0.007068:0.008659:0.008659:0.003702:0.009010:0.008659:0.008001:0.006486
in:@0.192874:0.564184:0.205235:0.564184:0.205235:0.551159:0.192874:0.551159:0.003702:0.008659
this:@0.296594:0.564184:0.320627:0.564184:0.320627:0.551159:0.296594:0.551159:0.005186:0.008659:0.003702:0.006486
.:@0.416499:0.564184:0.419818:0.564184:0.419818:0.551159:0.416499:0.551159:0.003320
 :@0.073142:0.592518:0.077333:0.592518:0.077333:0.579492:0.073142:0.579492:0.004192
  c.  :@0.109887:0.592518:0.141829:0.592518:0.141829:0.579492:0.109887:0.579492:0.004192:0.008980:0.007068:0.003320:0.004192:0.004192
my:@0.208770:0.599221:0.229346:0.599221:0.229346:0.586196:0.208770:0.586196:0.013171:0.007404
first :@0.359179:0.599221:0.388857:0.599221:0.388857:0.586196:0.359179:0.586196:0.004788:0.003702:0.005324:0.006486:0.005186:0.004192
experience:@0.488400:0.599221:0.561173:0.599221:0.561173:0.586196:0.488400:0.586196:0.008001:0.007022:0.008995:0.008001:0.005324:0.003702:0.008001:0.008659:0.007068:0.008001
Text:@0.664281:0.599221:0.691010:0.599221:0.691010:0.586196:0.664281:0.586196:0.006521:0.008001:0.007022:0.005186
Mining:@0.806765:0.599221:0.854235:0.599221:0.854235:0.586196:0.806765:0.586196:0.013738:0.003702:0.008659:0.003702:0.008659:0.009010
learnt:@0.199744:0.614762:0.238402:0.614762:0.238402:0.601736:0.199744:0.601736:0.003702:0.008001:0.007787:0.005324:0.008659:0.005186
new:@0.358078:0.614762:0.385797:0.614762:0.385797:0.601736:0.358078:0.601736:0.008659:0.008001:0.011060
techniques :@0.488079:0.614762:0.565701:0.614762:0.565701:0.601736:0.488079:0.601736:0.005186:0.008001:0.007068:0.008659:0.008659:0.003702:0.009010:0.008659:0.008001:0.006486:0.004192
 :@0.072850:0.648073:0.077042:0.648073:0.077042:0.635047:0.072850:0.635047:0.004192
  d.  this is my first experience of text mining. i have learnt new techniques in this:@0.109888:0.648073:0.656267:0.648073:0.656267:0.635047:0.109888:0.635047:0.004192:0.008688:0.009010:0.003320:0.004192:0.004192:0.005186:0.008659:0.003702:0.006486:0.004192:0.003702:0.006486:0.004192:0.013171:0.007404:0.004192:0.004788:0.003702:0.005324:0.006486:0.005186:0.004192:0.008001:0.007022:0.008995:0.008001:0.005324:0.003702:0.008001:0.008659:0.007068:0.008001:0.004192:0.008965:0.004788:0.004192:0.005186:0.008001:0.007022:0.005186:0.004192:0.013171:0.003702:0.008659:0.003702:0.008659:0.009010:0.003320:0.004192:0.003702:0.004192:0.008659:0.007787:0.007328:0.008001:0.004192:0.003702:0.008001:0.007787:0.005324:0.008659:0.005186:0.004192:0.008659:0.008001:0.011060:0.004192:0.005186:0.008001:0.007068:0.008659:0.008659:0.003702:0.009010:0.008659:0.008001:0.006486:0.004192:0.003702:0.008659:0.004192:0.005186:0.008659:0.003702:0.006486
 :@0.072850:0.670091:0.077042:0.670091:0.077042:0.657065:0.072850:0.657065:0.004192
4.  For the given corpus::@0.101641:0.670091:0.264457:0.670091:0.264457:0.657065:0.101641:0.657065:0.008246:0.003320:0.004192:0.005370:0.007465:0.008965:0.005324:0.004192:0.005186:0.008659:0.008001:0.004192:0.009010:0.003702:0.007328:0.008001:0.008659:0.004192:0.007068:0.008965:0.005324:0.008995:0.008659:0.006486:0.003320
Document 1::@0.122770:0.688769:0.214649:0.688769:0.214649:0.675687:0.122770:0.675687:0.011275:0.009347:0.007343:0.009255:0.014013:0.008276:0.009255:0.005951:0.004222:0.008796:0.004146
 Amit and Amita are twins.:@0.214662:0.688610:0.392240:0.688610:0.392240:0.675584:0.214662:0.675584:0.004192:0.009867:0.013171:0.003702:0.005186:0.004192:0.007787:0.008659:0.009010:0.004192:0.009867:0.013171:0.003702:0.005186:0.007787:0.004192:0.007787:0.005324:0.008001:0.004192:0.005186:0.011060:0.003702:0.008659:0.006486:0.003320
Document 2::@0.122770:0.707288:0.214649:0.707288:0.214649:0.694205:0.122770:0.694205:0.011275:0.009347:0.007343:0.009255:0.014013:0.008276:0.009255:0.005951:0.004222:0.008796:0.004146
 Amit lives with his grandparents in Shimla.:@0.214662:0.707128:0.503654:0.707128:0.503654:0.694102:0.214662:0.694102:0.004192:0.009867:0.013171:0.003702:0.005186:0.004192:0.003702:0.003702:0.007328:0.008001:0.006486:0.004192:0.011060:0.003702:0.005186:0.008659:0.004192:0.008659:0.003702:0.006486:0.004192:0.009010:0.005324:0.007787:0.008659:0.009010:0.008995:0.007787:0.005324:0.008001:0.008659:0.005186:0.006486:0.004192:0.003702:0.008659:0.004192:0.008123:0.008659:0.003702:0.013171:0.003702:0.007787:0.003320
Document 3::@0.122770:0.725806:0.214649:0.725806:0.214649:0.712724:0.122770:0.712724:0.011275:0.009347:0.007343:0.009255:0.014013:0.008276:0.009255:0.005951:0.004222:0.008796:0.004146
 Amita lives with her parents in Delhi.:@0.214662:0.725647:0.464431:0.725647:0.464431:0.712621:0.214662:0.712621:0.004192:0.009867:0.013171:0.003702:0.005186:0.007787:0.004192:0.003702:0.003702:0.007328:0.008001:0.006486:0.004192:0.011060:0.003702:0.005186:0.008659:0.004192:0.008659:0.008001:0.005324:0.004192:0.008995:0.007787:0.005324:0.008001:0.008659:0.005186:0.006486:0.004192:0.003702:0.008659:0.004192:0.010724:0.008001:0.003702:0.008659:0.003702:0.003320
 :@0.072850:0.744165:0.077042:0.744165:0.077042:0.731140:0.072850:0.731140:0.004192
  Create a step-by-step Document Vector Table.:@0.109888:0.744165:0.432825:0.744165:0.432825:0.731140:0.109888:0.731140:0.004192:0.008688:0.009469:0.005324:0.008001:0.007787:0.005186:0.008001:0.004192:0.007787:0.004192:0.006486:0.005186:0.008001:0.008995:0.006119:0.008995:0.007404:0.006119:0.006486:0.005186:0.008001:0.008995:0.004192:0.010724:0.008965:0.007068:0.008659:0.013171:0.008001:0.008659:0.005186:0.004192:0.008507:0.008001:0.007068:0.005186:0.008965:0.005324:0.004192:0.006286:0.007787:0.008995:0.003702:0.008001:0.003320
  Ans. :@0.072850:0.770918:0.117306:0.770918:0.117306:0.757892:0.072850:0.757892:0.004192:0.007741:0.009867:0.008659:0.006486:0.003320:0.004192
Step 1:@0.125476:0.770477:0.176698:0.770954:0.176698:0.757424:0.125476:0.756947:0.007923:0.004735:0.008051:0.008928:0.015910:0.145381
 :@0.183977:0.771527:0.188644:0.771527:0.188644:0.757068:0.183977:0.757068:0.004667
Text Normalisation:@0.188792:0.770918:0.314591:0.770918:0.314591:0.757892:0.188792:0.757892:0.006521:0.008001:0.007022:0.005186:0.004192:0.011443:0.008965:0.005324:0.013171:0.007787:0.003702:0.003702:0.006486:0.007787:0.005186:0.003702:0.008965:0.008659
Document 1::@0.122770:0.789596:0.214649:0.789596:0.214649:0.776513:0.122770:0.776513:0.011275:0.009347:0.007343:0.009255:0.014013:0.008276:0.009255:0.005951:0.004222:0.008796:0.004146
 [Amit, and, Amita, are, twins]:@0.214662:0.789436:0.411439:0.789436:0.411439:0.776410:0.214662:0.776410:0.004192:0.004620:0.009867:0.013171:0.003702:0.005186:0.003320:0.004192:0.007787:0.008659:0.009010:0.003320:0.004192:0.009867:0.013171:0.003702:0.005186:0.007787:0.003320:0.004192:0.007787:0.005324:0.008001:0.003320:0.004192:0.005186:0.011060:0.003702:0.008659:0.006486:0.004620
Document 2::@0.122770:0.808114:0.214649:0.808114:0.214649:0.795032:0.122770:0.795032:0.011275:0.009347:0.007343:0.009255:0.014013:0.008276:0.009255:0.005951:0.004222:0.008796:0.004146
 [Amit, lives, with, his, grandparents, in Shimla]:@0.214662:0.807955:0.526173:0.807955:0.526173:0.794929:0.214662:0.794929:0.004192:0.004620:0.009867:0.013171:0.003702:0.005186:0.003320:0.004192:0.003702:0.003702:0.007328:0.008001:0.006486:0.003320:0.004192:0.011060:0.003702:0.005186:0.008659:0.003320:0.004192:0.008659:0.003702:0.006486:0.003320:0.004192:0.009010:0.005324:0.007787:0.008659:0.009010:0.008995:0.007787:0.005324:0.008001:0.008659:0.005186:0.006486:0.003320:0.004192:0.003702:0.008659:0.004192:0.008123:0.008659:0.003702:0.013171:0.003702:0.007787:0.004620
Document 3::@0.122770:0.826633:0.214649:0.826633:0.214649:0.813550:0.122770:0.813550:0.011275:0.009347:0.007343:0.009255:0.014013:0.008276:0.009255:0.005951:0.004222:0.008796:0.004146
 [Amita, lives, with, her, parents, in Delhi]:@0.214662:0.826473:0.486949:0.826473:0.486949:0.813447:0.214662:0.813447:0.004192:0.004620:0.009867:0.013171:0.003702:0.005186:0.007787:0.003320:0.004192:0.003702:0.003702:0.007328:0.008001:0.006486:0.003320:0.004192:0.011060:0.003702:0.005186:0.008659:0.003320:0.004192:0.008659:0.008001:0.005324:0.003320:0.004192:0.008995:0.007787:0.005324:0.008001:0.008659:0.005186:0.006486:0.003320:0.004192:0.003702:0.008659:0.004192:0.010724:0.008001:0.003702:0.008659:0.003702:0.004620
Step 2:@0.125476:0.848050:0.177974:0.848527:0.177974:0.834998:0.125476:0.834521:0.007923:0.004735:0.008051:0.008928:0.014651:0.045457
 :@0.183977:0.849101:0.188644:0.849101:0.188644:0.834642:0.183977:0.834642:0.004667
Create Dictionary:@0.188792:0.848491:0.305271:0.848491:0.305271:0.835465:0.188792:0.835465:0.009469:0.005324:0.008001:0.007787:0.005186:0.008001:0.004192:0.010724:0.003702:0.007068:0.005186:0.003702:0.008965:0.008659:0.007787:0.005324:0.007404
Amit:@0.173363:0.869585:0.205289:0.869585:0.205289:0.856559:0.173363:0.856559:0.009867:0.013171:0.003702:0.005186
and:@0.308113:0.869585:0.333568:0.869585:0.333568:0.856559:0.308113:0.856559:0.007787:0.008659:0.009010
Amita:@0.432487:0.869585:0.472201:0.869585:0.472201:0.856559:0.432487:0.856559:0.009867:0.013171:0.003702:0.005186:0.007787
are:@0.573303:0.869585:0.594414:0.869585:0.594414:0.856559:0.573303:0.856559:0.007787:0.005324:0.008001
twins:@0.697816:0.869585:0.732909:0.869585:0.732909:0.856559:0.697816:0.856559:0.005186:0.011060:0.003702:0.008659:0.006486
lives:@0.832256:0.869585:0.861475:0.869585:0.861475:0.856559:0.832256:0.856559:0.003702:0.003702:0.007328:0.008001:0.006486
with:@0.175028:0.890211:0.203635:0.890211:0.203635:0.877185:0.175028:0.877185:0.011060:0.003702:0.005186:0.008659
his:@0.311411:0.890211:0.330258:0.890211:0.330258:0.877185:0.311411:0.877185:0.008659:0.003702:0.006486
grandparents:@0.407244:0.890211:0.497471:0.890211:0.497471:0.877185:0.407244:0.877185:0.009010:0.005324:0.007787:0.008659:0.009010:0.008995:0.007787:0.005324:0.008001:0.008659:0.005186:0.006486
in:@0.577673:0.890211:0.590033:0.890211:0.590033:0.877185:0.577673:0.877185:0.003702:0.008659
Shimla:@0.692785:0.890211:0.737929:0.890211:0.737929:0.877185:0.692785:0.877185:0.008123:0.008659:0.003702:0.013171:0.003702:0.007787
her:@0.835883:0.890211:0.857866:0.890211:0.857866:0.877185:0.835883:0.877185:0.008659:0.008001:0.005324
parents:@0.164115:0.910838:0.214552:0.910838:0.214552:0.897812:0.164115:0.897812:0.008995:0.007787:0.005324:0.008001:0.008659:0.005186:0.006486
Delhi:@0.303440:0.910838:0.338228:0.910838:0.338228:0.897812:0.303440:0.897812:0.010724:0.008001:0.003702:0.008659:0.003702