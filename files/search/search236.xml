T:@0.180169:0.948923:0.188607:0.948923:0.188607:0.935211:0.180169:0.935211:0.008438
ouchpad Artificial Intelligence (Ver. 3.0):@0.187317:0.948923:0.463822:0.948083:0.463822:0.934372:0.187317:0.935211:0.009436:0.009114:0.007440:0.009114:0.009469:0.008196:0.009485:0.004412:0.010386:0.005604:0.005459:0.003897:0.004469:0.004469:0.007440:0.003897:0.008196:0.003897:0.004412:0.004283:0.009114:0.005459:0.008422:0.003897:0.003897:0.003897:0.009485:0.008422:0.009114:0.007440:0.008422:0.004412:0.004863:0.008952:0.008422:0.005604:0.003494:0.004412:0.008680:0.003494:0.008665:-0.373357
-X:@0.463821:0.948260:0.480037:0.948926:0.480037:0.935215:0.463821:0.934548:0.006715:-0.386033
234:@0.118726:0.949910:0.144765:0.949910:0.144765:0.936198:0.118726:0.936198:0.008680:0.008680:0.008680
Classification Accuracy = :@0.253892:0.078286:0.442638:0.078286:0.442638:0.063889:0.253892:0.063889:0.010466:0.004092:0.008606:0.007169:0.007169:0.004092:0.004692:0.004692:0.007812:0.008606:0.005732:0.004092:0.009908:0.009570:0.004633:0.010906:0.007812:0.007812:0.009570:0.005884:0.008606:0.007812:0.008184:0.004633:0.011565:0.004633
Total no. of correct predictions:@0.449401:0.070200:0.676704:0.070200:0.676704:0.055803:0.449401:0.055803:0.007208:0.009908:0.005732:0.008606:0.004092:0.004633:0.009570:0.009908:0.003669:0.004633:0.009908:0.005292:0.004633:0.007812:0.009908:0.005884:0.005884:0.008843:0.007812:0.005732:0.004633:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009908:0.009570:0.007169
Total no. of predictions:@0.477638:0.088724:0.648433:0.088724:0.648433:0.074327:0.477638:0.074327:0.007208:0.009908:0.005732:0.008606:0.004092:0.004633:0.009570:0.009908:0.003669:0.004633:0.009908:0.005292:0.004633:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009908:0.009570:0.007169
 × 100:@0.683442:0.078286:0.731613:0.078286:0.731613:0.063889:0.683442:0.063889:0.004633:0.011565:0.004633:0.009114:0.009114:0.009114
=:@0.428339:0.117368:0.438803:0.117368:0.438803:0.104342:0.428339:0.104342:0.010464
(TP + TN):@0.481935:0.108418:0.548233:0.108418:0.548233:0.091553:0.481935:0.091553:0.005140:0.008894:0.009722:0.002114:0.011582:0.002114:0.008894:0.012698:0.005140
(TP + TN + FP + FN):@0.446648:0.126942:0.583520:0.126942:0.583520:0.110077:0.446648:0.110077:0.005140:0.008894:0.009722:0.002114:0.011582:0.002114:0.008894:0.012698:0.002114:0.011582:0.002114:0.008268:0.009722:0.002114:0.011582:0.002114:0.008268:0.012698:0.005140
 × 100:@0.587177:0.116504:0.635400:0.116504:0.635400:0.099639:0.587177:0.099639:0.004650:0.011582:0.004650:0.009114:0.009114:0.009114
=:@0.428339:0.156884:0.438803:0.156884:0.438803:0.143858:0.428339:0.143858:0.010464
25:@0.446285:0.149910:0.464512:0.149910:0.464512:0.135513:0.446285:0.135513:0.009114:0.009114
51:@0.446285:0.168434:0.464512:0.168434:0.464512:0.154037:0.446285:0.154037:0.009114:0.009114
 :@0.467801:0.156884:0.471993:0.156884:0.471993:0.143858:0.467801:0.143858:0.004192
× 100:@0.471992:0.157318:0.515531:0.157318:0.515531:0.142920:0.471992:0.142920:0.011565:0.004633:0.009114:0.009114:0.009114
=:@0.428339:0.189401:0.438803:0.189401:0.438803:0.176375:0.428339:0.176375:0.010464
49%:@0.442997:0.189834:0.475055:0.189834:0.475055:0.175437:0.442997:0.175437:0.009114:0.009114:0.013831
Can we use Accuracy all the time?:@0.072464:0.226380:0.453502:0.226380:0.453502:0.199288:0.072464:0.199288:0.015072:0.012995:0.014614:0.006667:0.019251:0.013068:0.006667:0.014614:0.010628:0.013068:0.006667:0.016981:0.011594:0.011594:0.014614:0.009614:0.012995:0.011594:0.012995:0.006667:0.012995:0.006860:0.006860:0.006667:0.009396:0.014541:0.013068:0.006667:0.009396:0.006860:0.022126:0.013068:0.010580
It is suitable wherever the dataset is balanced, which means the positive and negative classes are roughly equal, that :@0.072464:0.247042:0.917744:0.247042:0.917744:0.232645:0.072464:0.232645:0.004329:0.005563:0.004269:0.003923:0.007000:0.004269:0.007000:0.009401:0.003923:0.005563:0.008437:0.009773:0.003923:0.008674:0.004269:0.012056:0.009401:0.008674:0.005715:0.008674:0.007930:0.008674:0.005715:0.004269:0.005563:0.009401:0.008674:0.004271:0.009790:0.008437:0.005563:0.008437:0.007000:0.008674:0.005563:0.004269:0.003923:0.007000:0.004269:0.009773:0.008437:0.003923:0.008437:0.009401:0.007643:0.008674:0.009790:0.003500:0.004269:0.012056:0.009401:0.003923:0.007643:0.009401:0.004269:0.014389:0.008674:0.008437:0.009401:0.007000:0.004269:0.005563:0.009401:0.008674:0.004269:0.009773:0.009739:0.007000:0.003923:0.005563:0.003923:0.007930:0.008674:0.004269:0.008437:0.009401:0.009790:0.004271:0.009401:0.008674:0.009790:0.008437:0.005563:0.003923:0.007930:0.008674:0.004271:0.007643:0.003923:0.008437:0.007000:0.007000:0.008674:0.007000:0.004271:0.008437:0.005715:0.008674:0.004271:0.005715:0.009739:0.009401:0.009790:0.009401:0.003923:0.008014:0.004269:0.008674:0.009790:0.009401:0.008437:0.003923:0.003500:0.004269:0.005563:0.009401:0.008437:0.005729:0.004633
is a rare occurrence, and that all predictions and prediction errors are equally important, which is often not the case. :@0.072464:0.265561:0.917805:0.265561:0.917805:0.251164:0.072464:0.251164:0.003923:0.007000:0.004427:0.008437:0.004427:0.005715:0.008437:0.005715:0.008674:0.004427:0.009739:0.007643:0.007643:0.009401:0.005715:0.005715:0.008674:0.009401:0.007643:0.008674:0.003500:0.004427:0.008437:0.009401:0.009790:0.004427:0.005563:0.009401:0.008437:0.005563:0.004428:0.008437:0.003923:0.003923:0.004428:0.009773:0.005715:0.008674:0.009790:0.003923:0.007643:0.005563:0.003923:0.009739:0.009401:0.007000:0.004427:0.008437:0.009401:0.009790:0.004427:0.009773:0.005715:0.008674:0.009790:0.003923:0.007643:0.005563:0.003923:0.009739:0.009401:0.004428:0.008674:0.005715:0.005715:0.009739:0.005715:0.007000:0.004428:0.008437:0.005715:0.008674:0.004428:0.008674:0.009790:0.009401:0.008437:0.003923:0.003923:0.008014:0.004428:0.003923:0.014389:0.009773:0.009739:0.005715:0.005563:0.008437:0.009401:0.005563:0.003500:0.004428:0.012056:0.009401:0.003923:0.007643:0.009401:0.004428:0.003923:0.007000:0.004427:0.009739:0.005123:0.005563:0.008674:0.009401:0.004428:0.009401:0.009739:0.005563:0.004427:0.005563:0.009401:0.008674:0.004427:0.007643:0.008437:0.007000:0.008674:0.003666:0.004633
For example, Calculating the accuracy of the classifier model, that predicts whether a student will pass a test :@0.072464:0.287579:0.917682:0.287579:0.917682:0.273182:0.072464:0.273182:0.008251:0.009908:0.005884:0.006907:0.008843:0.007761:0.008606:0.014558:0.009942:0.004092:0.008843:0.003669:0.006912:0.010466:0.008606:0.004092:0.007812:0.009570:0.004092:0.008606:0.005732:0.004092:0.009570:0.009959:0.006905:0.005732:0.009570:0.008843:0.006905:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.007812:0.008184:0.006888:0.009908:0.005292:0.006907:0.005732:0.009570:0.008843:0.006905:0.007812:0.004092:0.008606:0.007169:0.007169:0.004092:0.004692:0.004692:0.008843:0.005884:0.006909:0.014558:0.009908:0.009959:0.008843:0.004092:0.003669:0.006910:0.005732:0.009570:0.008606:0.005732:0.006900:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.007169:0.006904:0.012225:0.009570:0.008843:0.005732:0.009570:0.008843:0.005884:0.006890:0.008606:0.006905:0.007169:0.005732:0.009570:0.009959:0.008843:0.009570:0.005732:0.006905:0.012225:0.004092:0.004092:0.004092:0.006912:0.009942:0.008606:0.007169:0.007169:0.006915:0.008606:0.006905:0.005732:0.008843:0.007169:0.005732:0.004633
(Yes) or not pass a test (No). It classifies the input into two classes :@0.072464:0.306098:0.564357:0.306098:0.564357:0.291700:0.072464:0.291700:0.005106:0.007856:0.008843:0.007169:0.005106:0.004704:0.009908:0.005884:0.004697:0.009570:0.009908:0.005732:0.004700:0.009942:0.008606:0.007169:0.007169:0.004716:0.008606:0.004700:0.005732:0.008843:0.007169:0.005732:0.004704:0.005106:0.012647:0.009908:0.005106:0.003657:0.004704:0.004498:0.005732:0.004704:0.007812:0.004092:0.008606:0.007169:0.007169:0.004092:0.004692:0.004692:0.008843:0.007169:0.004714:0.005732:0.009570:0.008843:0.004700:0.004092:0.009570:0.009942:0.009570:0.005732:0.004700:0.004092:0.009570:0.005732:0.009908:0.004702:0.005732:0.012216:0.009908:0.004704:0.007812:0.004092:0.008606:0.007169:0.007169:0.008843:0.007169:0.004633
Yes:@0.564434:0.306274:0.589763:0.306274:0.589763:0.291814:0.564434:0.291814:0.008742:0.009147:0.007440
 and :@0.589764:0.306098:0.627232:0.306098:0.627232:0.291700:0.589764:0.291700:0.004700:0.008606:0.009570:0.009959:0.004633
No:@0.627300:0.306274:0.650988:0.306274:0.650988:0.291814:0.627300:0.291814:0.013357:0.010331
. Let's, calculate the accuracy of the :@0.650994:0.306098:0.917681:0.306098:0.917681:0.291700:0.650994:0.291700:0.003669:0.004700:0.007964:0.008843:0.005732:0.003889:0.007169:0.003669:0.004697:0.007812:0.008606:0.004092:0.007812:0.009570:0.004092:0.008606:0.005732:0.008843:0.004695:0.005732:0.009570:0.008843:0.004700:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.007812:0.008184:0.004684:0.009908:0.005292:0.004702:0.005732:0.009570:0.008843:0.004633
classifier model and construct the confusion matrix for the model.:@0.072464:0.324616:0.561821:0.324616:0.561821:0.310219:0.072464:0.310219:0.007812:0.004092:0.008606:0.007169:0.007169:0.004092:0.004692:0.004692:0.008843:0.005884:0.004633:0.014558:0.009908:0.009959:0.008843:0.004092:0.004633:0.008606:0.009570:0.009959:0.004633:0.007812:0.009908:0.009570:0.007169:0.005732:0.005884:0.009570:0.007812:0.005732:0.004633:0.005732:0.009570:0.008843:0.004633:0.007812:0.009908:0.009570:0.005292:0.009570:0.007169:0.004092:0.009908:0.009570:0.004633:0.014558:0.008606:0.005732:0.005884:0.004092:0.007761:0.004633:0.005292:0.009908:0.005884:0.004633:0.005732:0.009570:0.008843:0.004633:0.014558:0.009908:0.009959:0.008843:0.004092:0.003669
Here,:@0.072464:0.346634:0.111708:0.346634:0.111708:0.332237:0.072464:0.332237:0.012005:0.008843:0.005884:0.008843:0.003669
 :@0.080241:0.368652:0.084874:0.368652:0.084874:0.354255:0.080241:0.354255:0.004633
•:@0.072464:0.366952:0.080242:0.366952:0.080242:0.351073:0.072464:0.351073:0.007778
Total test data is 1000.:@0.090722:0.368652:0.256563:0.368652:0.256563:0.354255:0.090722:0.354255:0.007208:0.009908:0.005732:0.008606:0.004092:0.004633:0.005732:0.008843:0.007169:0.005732:0.004633:0.009959:0.008606:0.005732:0.008606:0.004633:0.004092:0.007169:0.004633:0.009114:0.009114:0.009114:0.009114:0.003669
 :@0.080241:0.390670:0.084874:0.390670:0.084874:0.376273:0.080241:0.376273:0.004633
•:@0.072464:0.388971:0.080242:0.388971:0.080242:0.373091:0.072464:0.373091:0.007778
Actual values are 900 :@0.090722:0.390670:0.253024:0.390670:0.253024:0.376273:0.090722:0.376273:0.010906:0.007812:0.005732:0.009570:0.008606:0.004092:0.004633:0.008099:0.008606:0.004092:0.009570:0.008843:0.007169:0.004633:0.008606:0.005884:0.008843:0.004633:0.009114:0.009114:0.009114:0.004633
Yes:@0.253010:0.390847:0.278338:0.390847:0.278338:0.376387:0.253010:0.376387:0.008742:0.009147:0.007440
 and 100 :@0.278339:0.390670:0.347714:0.390670:0.347714:0.376273:0.278339:0.376273:0.004633:0.008606:0.009570:0.009959:0.004633:0.009114:0.009114:0.009114:0.004633
No:@0.347706:0.390847:0.371394:0.390847:0.371394:0.376387:0.347706:0.376387:0.013357:0.010331
 (Unbalanced dataset). :@0.371400:0.390670:0.542444:0.390670:0.542444:0.376273:0.371400:0.376273:0.004633:0.005106:0.011616:0.009570:0.009942:0.008606:0.004092:0.008606:0.009570:0.007812:0.008843:0.009959:0.004633:0.009959:0.008606:0.005732:0.008606:0.007169:0.008843:0.005732:0.005106:0.003669:0.004633
 :@0.080241:0.412689:0.084874:0.412689:0.084874:0.398291:0.080241:0.398291:0.004633
•:@0.072464:0.410989:0.080242:0.410989:0.080242:0.395109:0.072464:0.395109:0.007778
It is a faulty model which, irrespective of any input, will give a prediction as :@0.090722:0.412689:0.651162:0.412689:0.651162:0.398291:0.090722:0.398291:0.004498:0.005732:0.004633:0.004092:0.007169:0.004633:0.008606:0.004633:0.005292:0.008606:0.009570:0.004092:0.005732:0.008184:0.004633:0.014558:0.009908:0.009959:0.008843:0.004092:0.004633:0.012225:0.009570:0.004092:0.007812:0.009570:0.003669:0.004633:0.004092:0.005884:0.005884:0.008843:0.007169:0.009942:0.008843:0.007812:0.005732:0.004092:0.008099:0.008843:0.004633:0.009908:0.005292:0.004633:0.008606:0.009570:0.008184:0.004633:0.004092:0.009570:0.009942:0.009570:0.005732:0.003669:0.004633:0.012225:0.004092:0.004092:0.004092:0.004633:0.009959:0.004092:0.008099:0.008843:0.004633:0.008606:0.004633:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004092:0.009908:0.009570:0.004633:0.008606:0.007169:0.004633
Yes:@0.651104:0.412865:0.676433:0.412865:0.676433:0.398405:0.651104:0.398405:0.008742:0.009147:0.007440
.:@0.676433:0.412689:0.680103:0.412689:0.680103:0.398291:0.676433:0.398291:0.003669
 :@0.080241:0.434707:0.084874:0.434707:0.084874:0.420310:0.080241:0.420310:0.004633
•:@0.072464:0.433007:0.080242:0.433007:0.080242:0.417127:0.072464:0.417127:0.007778
Calculate the classification accuracy of this model.:@0.090722:0.434707:0.461604:0.434707:0.461604:0.420310:0.090722:0.420310:0.010466:0.008606:0.004092:0.007812:0.009570:0.004092:0.008606:0.005732:0.008843:0.004633:0.005732:0.009570:0.008843:0.004633:0.007812:0.004092:0.008606:0.007169:0.007169:0.004092:0.005292:0.004092:0.007812:0.008606:0.005732:0.004092:0.009908:0.009570:0.004633:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.007812:0.008184:0.004633:0.009908:0.005292:0.004633:0.005732:0.009570:0.004092:0.007169:0.004633:0.014558:0.009908:0.009959:0.008843:0.004092:0.003669
To prepare the classification accuracy of this model follow the given steps::@0.072464:0.456725:0.622831:0.456725:0.622831:0.442328:0.072464:0.442328:0.007208:0.009908:0.004633:0.009942:0.005884:0.008843:0.009942:0.008606:0.005884:0.008843:0.004633:0.005732:0.009570:0.008843:0.004633:0.007812:0.004092:0.008606:0.007169:0.007169:0.004092:0.004692:0.004692:0.007812:0.008606:0.005732:0.004092:0.009908:0.009570:0.004633:0.008606:0.007812:0.007812:0.009570:0.005884:0.008606:0.007812:0.008184:0.004633:0.009908:0.005292:0.004633:0.005732:0.009570:0.004092:0.007169:0.004633:0.014558:0.009908:0.009959:0.008843:0.004092:0.004633:0.005292:0.009908:0.004092:0.004092:0.009908:0.012225:0.004633:0.005732:0.009570:0.008843:0.004633:0.009959:0.004092:0.008099:0.008843:0.009570:0.004633:0.007169:0.005732:0.008843:0.009942:0.007169:0.003669
Step 1:@0.075471:0.485493:0.132384:0.486022:0.132384:0.470989:0.075471:0.470460:0.008804:0.005261:0.008945:0.009919:0.017678:0.563935
 :@0.140472:0.485426:0.146649:0.485426:0.146649:0.461004:0.140472:0.461004:0.006177
  Construct the Actual value vs Predicted value table. Consider :@0.149758:0.484465:0.615098:0.484465:0.615098:0.470067:0.149758:0.470067:0.004633:-0.004633:0.010466:0.009908:0.009570:0.007169:0.005732:0.005884:0.009570:0.007812:0.005732:0.005865:0.005732:0.009570:0.008843:0.005867:0.010906:0.007812:0.005732:0.009570:0.008606:0.004092:0.005869:0.008099:0.008606:0.004092:0.009570:0.008843:0.005869:0.008099:0.007169:0.005879:0.009469:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.008843:0.009959:0.005860:0.008099:0.008606:0.004092:0.009570:0.008843:0.005872:0.005732:0.008606:0.009942:0.004092:0.008843:0.003669:0.005864:0.010466:0.009908:0.009570:0.007169:0.004092:0.009959:0.008843:0.005884:0.004633
Yes:@0.616336:0.484641:0.641664:0.484641:0.641664:0.470181:0.616336:0.470181:0.008742:0.009147:0.007440
 as the positive class and :@0.641665:0.484465:0.836439:0.484465:0.836439:0.470067:0.641665:0.470067:0.005867:0.008606:0.007169:0.005867:0.005732:0.009570:0.008843:0.005867:0.009942:0.009908:0.007169:0.004092:0.005732:0.004092:0.008099:0.008843:0.005879:0.007812:0.004092:0.008606:0.007169:0.007169:0.005881:0.008606:0.009570:0.009959:0.004633
No:@0.837686:0.484641:0.861374:0.484641:0.861374:0.470181:0.837686:0.470181:0.013357:0.010331
 as the :@0.861381:0.484465:0.917668:0.484465:0.917668:0.470067:0.861381:0.470067:0.005867:0.008606:0.007169:0.005867:0.005732:0.009570:0.008843:0.004633
negative class.:@0.149759:0.502983:0.256652:0.502983:0.256652:0.488586:0.149759:0.488586:0.009570:0.008843:0.009959:0.008606:0.005732:0.004092:0.008099:0.008843:0.004633:0.007812:0.004092:0.008606:0.007169:0.007169:0.003669
Predicted Value:@0.343747:0.536742:0.467408:0.536742:0.467408:0.522282:0.343747:0.522282:0.010382:0.006667:0.009147:0.010466:0.004802:0.008116:0.006520:0.009147:0.010466:0.004667:0.010006:0.009097:0.004802:0.010229:0.009147
Actual Value:@0.523190:0.536742:0.621853:0.536742:0.621853:0.522282:0.523190:0.522282:0.011886:0.008116:0.006577:0.010229:0.009097:0.004802:0.004667:0.010013:0.009097:0.004802:0.010229:0.009147
Step 2:@0.075471:0.642257:0.133801:0.642787:0.133801:0.627754:0.075471:0.627224:0.008804:0.005261:0.008945:0.009919:0.016279:0.360859
 :@0.140472:0.642191:0.146649:0.642191:0.146649:0.617769:0.140472:0.617769:0.006177
Construct the confusion matrix.:@0.149759:0.641229:0.382838:0.641229:0.382838:0.626832:0.149759:0.626832:0.010466:0.009908:0.009570:0.007169:0.005732:0.005884:0.009570:0.007812:0.005732:0.004633:0.005732:0.009570:0.008843:0.004633:0.007812:0.009908:0.009570:0.005292:0.009570:0.007169:0.004092:0.009908:0.009570:0.004633:0.014558:0.008606:0.005732:0.005884:0.004092:0.007761:0.003669
 :@0.072464:0.663247:0.077097:0.663247:0.077097:0.648850:0.072464:0.648850:0.004633
       So, the faulty model will predict all the 1000 input data as Yes. :@0.109493:0.663247:0.606916:0.663247:0.606916:0.648850:0.109493:0.648850:0.004633:0.004633:0.004633:0.004633:0.004633:0.004633:0.004633:0.008978:0.009908:0.003669:0.004633:0.005732:0.009570:0.008843:0.004633:0.005292:0.008606:0.009570:0.004092:0.005732:0.008184:0.004633:0.014558:0.009908:0.009959:0.008843:0.004092:0.004633:0.012225:0.004092:0.004092:0.004092:0.004633:0.009942:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.004633:0.008606:0.004092:0.004092:0.004633:0.005732:0.009570:0.008843:0.004633:0.009114:0.009114:0.009114:0.009114:0.004633:0.004092:0.009570:0.009942:0.009570:0.005732:0.004633:0.009959:0.008606:0.005732:0.008606:0.004633:0.008606:0.007169:0.004633:0.007812:0.008843:0.007169:0.003669:0.004633
 :@0.072464:0.685265:0.077097:0.685265:0.077097:0.670868:0.072464:0.670868:0.004633
        Consider :@0.109493:0.685265:0.212430:0.685265:0.212430:0.670868:0.109493:0.670868:0.004633:0.004633:0.004633:0.004633:0.004633:0.004633:0.004633:0.000000:0.010466:0.009908:0.009570:0.007169:0.004092:0.009959:0.008843:0.005884:0.004633
Yes:@0.212399:0.685441:0.237728:0.685441:0.237728:0.670982:0.212399:0.670982:0.008742:0.009147:0.007440
 as the positive class and :@0.237729:0.685265:0.426076:0.685265:0.426076:0.670868:0.237729:0.670868:0.004582:0.008606:0.007169:0.004582:0.005732:0.009570:0.008843:0.004582:0.009942:0.009908:0.007169:0.004092:0.005732:0.004092:0.008099:0.008843:0.004592:0.007812:0.004092:0.008606:0.007169:0.007169:0.004596:0.008606:0.009570:0.009959:0.004633
No:@0.426031:0.685441:0.449720:0.685441:0.449720:0.670982:0.426031:0.670982:0.013357:0.010331
 as the negative class. Construct the confusion matrix from the :@0.449726:0.685265:0.917684:0.685265:0.917684:0.670868:0.449726:0.670868:0.004586:0.008606:0.007169:0.004589:0.005732:0.009570:0.008843:0.004586:0.009570:0.008843:0.009959:0.008606:0.005732:0.004092:0.008099:0.008843:0.004579:0.007812:0.004092:0.008606:0.007169:0.007169:0.003669:0.004591:0.010466:0.009908:0.009570:0.007169:0.005732:0.005884:0.009570:0.007812:0.005732:0.004579:0.005732:0.009570:0.008843:0.004582:0.007812:0.009908:0.009570:0.005292:0.009570:0.007169:0.004092:0.009908:0.009570:0.004587:0.014558:0.008606:0.005732:0.005884:0.004092:0.007761:0.004582:0.005292:0.005884:0.009908:0.014558:0.004586:0.005732:0.009570:0.008843:0.004633
Actual vs Predicted table.:@0.141922:0.703784:0.329282:0.703784:0.329282:0.689387:0.141922:0.689387:0.010906:0.007812:0.005732:0.009570:0.008606:0.004092:0.004633:0.008099:0.007169:0.004633:0.009469:0.005884:0.008843:0.009959:0.004092:0.007812:0.005732:0.008843:0.009959:0.004633:0.005732:0.008606:0.009942:0.004092:0.008843:0.003669
Predicted Value:@0.358117:0.730572:0.481779:0.730572:0.481779:0.716112:0.358117:0.716112:0.010382:0.006667:0.009147:0.010466:0.004802:0.008116:0.006520:0.009147:0.010466:0.004667:0.010006:0.009097:0.004802:0.010229:0.009147
Actual value:@0.516079:0.730572:0.613643:0.730572:0.613643:0.716112:0.516079:0.716112:0.011886:0.008116:0.006577:0.010229:0.009097:0.004802:0.004667:0.008914:0.009097:0.004802:0.010229:0.009147
Yes=1000:@0.384000:0.754838:0.455890:0.754838:0.455890:0.740440:0.384000:0.740440:0.007859:0.008843:0.007169:0.011565:0.009114:0.009114:0.009114:0.009114
Yes=900:@0.533471:0.754838:0.596247:0.754838:0.596247:0.740440:0.533471:0.740440:0.007859:0.008843:0.007169:0.011565:0.009114:0.009114:0.009114
No=0:@0.398332:0.779280:0.441566:0.779280:0.441566:0.764882:0.398332:0.764882:0.012647:0.009908:0.011565:0.009114
No=100:@0.534131:0.779280:0.595592:0.779280:0.595592:0.764882:0.534131:0.764882:0.012647:0.009908:0.011565:0.009114:0.009114:0.009114
Yes:@0.451781:0.827919:0.474518:0.827919:0.474518:0.814208:0.451781:0.814208:0.007488:0.008422:0.006828
No:@0.548753:0.827919:0.570235:0.827919:0.570235:0.814208:0.548753:0.814208:0.012045:0.009436
Yes:@0.416198:0.861702:0.416198:0.844270:0.398313:0.844270:0.398313:0.861702:0.000000:0.015910:0.006828
TP=:@0.444723:0.863318:0.481573:0.863318:0.481573:0.846793:0.444723:0.846793:0.011324:0.011865:0.013662
FN=:@0.540008:0.863318:0.578984:0.863318:0.578984:0.846793:0.540008:0.846793:0.010048:0.015266:0.013662
No:@0.416198:0.904702:0.416198:0.888233:0.398313:0.888233:0.398313:0.904702:0.000000:0.021481
FP=:@0.445360:0.907727:0.480935:0.907727:0.480935:0.891201:0.445360:0.891201:0.010048:0.011865:0.013662
TN=:@0.539369:0.907727:0.579621:0.907727:0.579621:0.891201:0.539369:0.891201:0.011324:0.015266:0.013662
Predicted Values:@0.455158:0.809282:0.586260:0.809282:0.586260:0.794823:0.455158:0.794823:0.010382:0.006667:0.009147:0.010466:0.004802:0.008116:0.006520:0.009147:0.010466:0.004667:0.010006:0.009097:0.004802:0.010229:0.009147:0.007440
Actual Values:@0.395112:0.917842:0.395112:0.836497:0.376252:0.836497:0.376252:0.917842:0.000000:0.000000:0.000000:0.000000:0.000000:0.000000:0.000000:0.065387:0.222562:0.000000:0.000000:0.000000:-0.181847