Evaluating Models:@0.688403:0.948923:0.818564:0.948923:0.818564:0.935211:0.688403:0.935211:0.008148:0.007713:0.008196:0.003897:0.009114:0.008196:0.005459:0.003897:0.009114:0.009485:0.004412:0.014461:0.009436:0.009485:0.008422:0.003897:0.006828
241:@0.853958:0.949910:0.879997:0.949910:0.879997:0.936198:0.853958:0.936198:0.008680:0.008680:0.008680
 :@0.086957:0.068212:0.091148:0.068212:0.091148:0.055186:0.086957:0.055186:0.004192
7.  What does \Error\ refer to in model evaluation?:@0.115747:0.068212:0.456692:0.068212:0.456692:0.055186:0.115747:0.055186:0.008246:0.003320:0.004192:0.005370:0.014288:0.008659:0.007787:0.005186:0.004192:0.009010:0.008965:0.008001:0.006486:0.004192:0.005997:0.007741:0.005324:0.005324:0.008965:0.005324:0.005997:0.004192:0.005324:0.008001:0.004788:0.008001:0.005324:0.004192:0.005186:0.008965:0.004192:0.003702:0.008659:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.003702:0.008965:0.008659:0.006853
a.  The difference between the model’s prediction and the actual outcome :@0.135266:0.090230:0.640448:0.090230:0.640448:0.077204:0.135266:0.077204:0.007787:0.003320:0.004192:0.007251:0.008016:0.008659:0.008001:0.004192:0.009010:0.003702:0.004788:0.004788:0.008001:0.005324:0.008001:0.008659:0.007068:0.008001:0.004192:0.008995:0.008001:0.005186:0.011060:0.008001:0.008001:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.003503:0.006486:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.003702:0.008965:0.008659:0.004192:0.007787:0.008659:0.009010:0.004192:0.005186:0.008659:0.008001:0.004192:0.007787:0.007068:0.005186:0.008659:0.007787:0.003702:0.004192:0.008965:0.008659:0.005186:0.007068:0.008965:0.013171:0.008001:0.004192
b.  The total number of correct predictions :@0.135266:0.115748:0.427058:0.115748:0.427058:0.102722:0.135266:0.102722:0.008995:0.003320:0.004192:0.006043:0.008016:0.008659:0.008001:0.004192:0.005186:0.008965:0.005186:0.007787:0.003702:0.004192:0.008659:0.008659:0.013171:0.008995:0.008001:0.005324:0.004192:0.008965:0.004788:0.004192:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.003702:0.008965:0.008659:0.006486:0.004192
c.  The total number of false positives in the model :@0.135266:0.141266:0.484272:0.141266:0.484272:0.128240:0.135266:0.128240:0.007068:0.003320:0.004192:0.007970:0.008016:0.008659:0.008001:0.004192:0.005186:0.008965:0.005186:0.007787:0.003702:0.004192:0.008659:0.008659:0.013171:0.008995:0.008001:0.005324:0.004192:0.008965:0.004788:0.004192:0.004788:0.007787:0.003702:0.006486:0.008001:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.006486:0.004192:0.003702:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192
d.  The percentage of false negatives in the model :@0.135266:0.166783:0.477786:0.166783:0.477786:0.153757:0.135266:0.153757:0.009010:0.003320:0.004192:0.006027:0.008016:0.008659:0.008001:0.004192:0.008995:0.008001:0.005324:0.007068:0.008001:0.008659:0.005186:0.007787:0.009010:0.008001:0.004192:0.008965:0.004788:0.004192:0.004788:0.007787:0.003702:0.006486:0.008001:0.004192:0.008659:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.006486:0.004192:0.003702:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192
 :@0.086957:0.193863:0.091148:0.193863:0.091148:0.180837:0.086957:0.180837:0.004192
8.  When is accuracy an appropriate metric to use?:@0.115747:0.193863:0.456281:0.193863:0.456281:0.180837:0.115747:0.180837:0.008246:0.003320:0.004192:0.005372:0.014288:0.008659:0.008001:0.008659:0.004192:0.003702:0.006486:0.004192:0.007787:0.007068:0.007068:0.008659:0.005324:0.007787:0.007068:0.007404:0.004192:0.007787:0.008659:0.004192:0.007787:0.008995:0.008995:0.005324:0.008965:0.008995:0.005324:0.003702:0.007787:0.005186:0.008001:0.004192:0.013171:0.008001:0.005186:0.005324:0.003702:0.007068:0.004192:0.005186:0.008965:0.004192:0.008659:0.006486:0.008001:0.006853
a.  When the dataset is highly unbalanced with a significant difference between positive and negative classes. :@0.135266:0.215881:0.876449:0.215881:0.876449:0.202855:0.135266:0.202855:0.007787:0.003320:0.004192:0.007251:0.014288:0.008659:0.008001:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.009010:0.007787:0.005186:0.007787:0.006486:0.008001:0.005186:0.004192:0.003702:0.006486:0.004192:0.008659:0.003702:0.009010:0.008659:0.003702:0.007404:0.004192:0.008659:0.008659:0.008995:0.007787:0.003702:0.007787:0.008659:0.007068:0.008001:0.009010:0.004192:0.011060:0.003702:0.005186:0.008659:0.004192:0.007787:0.004192:0.006486:0.003702:0.009010:0.008659:0.003702:0.004788:0.003702:0.007068:0.007787:0.008659:0.005186:0.004192:0.009010:0.003702:0.004788:0.004788:0.008001:0.005324:0.008001:0.008659:0.007068:0.008001:0.004192:0.008995:0.008001:0.005186:0.011060:0.008001:0.008001:0.008659:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.004192:0.007787:0.008659:0.009010:0.004192:0.008659:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.004192:0.007068:0.003702:0.007787:0.006486:0.006486:0.008001:0.006486:0.003320:0.004192
b.  When the dataset is balanced, and both positive and negative classes are nearly equal. :@0.135266:0.241399:0.743143:0.241399:0.743143:0.228373:0.135266:0.228373:0.008995:0.003320:0.004192:0.006043:0.014288:0.008659:0.008001:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.009010:0.007787:0.005186:0.007787:0.006486:0.008001:0.005186:0.004192:0.003702:0.006486:0.004192:0.008995:0.007787:0.003702:0.007787:0.008659:0.007068:0.008001:0.009010:0.003320:0.004192:0.007787:0.008659:0.009010:0.004192:0.008995:0.008965:0.005186:0.008659:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.004192:0.007787:0.008659:0.009010:0.004192:0.008659:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.004192:0.007068:0.003702:0.007787:0.006486:0.006486:0.008001:0.006486:0.004192:0.007787:0.005324:0.008001:0.004192:0.008659:0.008001:0.007787:0.005324:0.003702:0.007404:0.004192:0.008001:0.009010:0.008659:0.007787:0.003702:0.003320:0.004192
c.  When precision is the most important factor. :@0.135266:0.266916:0.462916:0.266916:0.462916:0.253890:0.135266:0.253890:0.007068:0.003320:0.004192:0.007970:0.014288:0.008659:0.008001:0.008659:0.004192:0.008995:0.005324:0.008001:0.007068:0.003702:0.006486:0.003702:0.008965:0.008659:0.004192:0.003702:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.006486:0.005186:0.004192:0.003702:0.013171:0.008995:0.008965:0.005324:0.005186:0.007787:0.008659:0.005186:0.004192:0.004788:0.007787:0.007068:0.005186:0.008965:0.005324:0.003320:0.004192
d.  When recall is the most important factor. :@0.135266:0.292434:0.437598:0.292434:0.437598:0.279408:0.135266:0.279408:0.009010:0.003320:0.004192:0.006027:0.014288:0.008659:0.008001:0.008659:0.004192:0.005324:0.008001:0.007068:0.007787:0.003702:0.003702:0.004192:0.003702:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.006486:0.005186:0.004192:0.003702:0.013171:0.008995:0.008965:0.005324:0.005186:0.007787:0.008659:0.005186:0.004192:0.004788:0.007787:0.007068:0.005186:0.008965:0.005324:0.003320:0.004192
 :@0.086957:0.318279:0.091148:0.318279:0.091148:0.305253:0.086957:0.305253:0.004192
9.  What does the classification accuracy of a model indicate?:@0.115747:0.318279:0.528240:0.318279:0.528240:0.305253:0.115747:0.305253:0.008246:0.003320:0.004192:0.005370:0.014288:0.008659:0.007787:0.005186:0.004192:0.009010:0.008965:0.008001:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.007068:0.003702:0.007787:0.006486:0.006486:0.003702:0.004788:0.003702:0.007068:0.007787:0.005186:0.003702:0.008965:0.008659:0.004192:0.007787:0.007068:0.007068:0.008659:0.005324:0.007787:0.007068:0.007404:0.004192:0.008965:0.004788:0.004192:0.007787:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.003702:0.008659:0.009010:0.003702:0.007068:0.007787:0.005186:0.008001:0.006853
a.  The ability of the model to classify negative cases :@0.135266:0.340297:0.494216:0.340297:0.494216:0.327271:0.135266:0.327271:0.007787:0.003320:0.004192:0.007251:0.008016:0.008659:0.008001:0.004192:0.007787:0.008995:0.003702:0.003702:0.003702:0.005186:0.007404:0.004192:0.008965:0.004788:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.005186:0.008965:0.004192:0.007068:0.003702:0.007787:0.006486:0.006486:0.003702:0.004788:0.007404:0.004192:0.008659:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.004192:0.007068:0.007787:0.006486:0.008001:0.006486:0.004192
b.  The number of false positives in the dataset :@0.135266:0.365815:0.455849:0.365815:0.455849:0.352789:0.135266:0.352789:0.008995:0.003320:0.004192:0.006043:0.008016:0.008659:0.008001:0.004192:0.008659:0.008659:0.013171:0.008995:0.008001:0.005324:0.004192:0.008965:0.004788:0.004192:0.004788:0.007787:0.003702:0.006486:0.008001:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.006486:0.004192:0.003702:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.009010:0.007787:0.005186:0.007787:0.006486:0.008001:0.005186:0.004192
c.  The proportion of incorrect predictions :@0.135266:0.391333:0.424671:0.391333:0.424671:0.378307:0.135266:0.378307:0.007068:0.003320:0.004192:0.007970:0.008016:0.008659:0.008001:0.004192:0.008995:0.005324:0.008965:0.008995:0.008965:0.005324:0.005186:0.003702:0.008965:0.008659:0.004192:0.008965:0.004788:0.004192:0.003702:0.008659:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.003702:0.008965:0.008659:0.006486:0.004192
d.  The percentage of correct predictions out of total predictions :@0.135266:0.416850:0.574514:0.416850:0.574514:0.403824:0.135266:0.403824:0.009010:0.003320:0.004192:0.006027:0.008016:0.008659:0.008001:0.004192:0.008995:0.008001:0.005324:0.007068:0.008001:0.008659:0.005186:0.007787:0.009010:0.008001:0.004192:0.008965:0.004788:0.004192:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.003702:0.008965:0.008659:0.006486:0.004192:0.008965:0.008659:0.005186:0.004192:0.008965:0.004788:0.004192:0.005186:0.008965:0.005186:0.007787:0.003702:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.003702:0.008965:0.008659:0.006486:0.004192
 :@0.086957:0.444547:0.091148:0.444547:0.091148:0.431521:0.086957:0.431521:0.004192
10.  Which metric is used to reduce the no. of false positives and false negatives?:@0.107502:0.444547:0.652138:0.444547:0.652138:0.431521:0.107502:0.431521:0.008246:0.008246:0.003320:0.004192:0.005370:0.014288:0.008659:0.003702:0.007068:0.008659:0.004192:0.013171:0.008001:0.005186:0.005324:0.003702:0.007068:0.004192:0.003702:0.006486:0.004192:0.008659:0.006486:0.008001:0.009010:0.004192:0.005186:0.008965:0.004192:0.005324:0.008001:0.009010:0.008659:0.007068:0.008001:0.004192:0.005186:0.008659:0.008001:0.004192:0.008659:0.008965:0.003320:0.004192:0.008965:0.004788:0.004192:0.004788:0.007787:0.003702:0.006486:0.008001:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.006486:0.004192:0.007787:0.008659:0.009010:0.004192:0.004788:0.007787:0.003702:0.006486:0.008001:0.004192:0.008659:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.006486:0.006853
a.  Accuracy :@0.135266:0.467183:0.222250:0.467183:0.222250:0.454157:0.135266:0.454157:0.007787:0.003320:0.004192:0.007251:0.009867:0.007068:0.007068:0.008659:0.005324:0.007787:0.007068:0.007404:0.004192
 :@0.455057:0.467183:0.459249:0.467183:0.459249:0.454157:0.455057:0.454157:0.004192
b.  F1 - Score :@0.544290:0.467183:0.637102:0.467183:0.637102:0.454157:0.544290:0.454157:0.008995:0.003320:0.004192:0.004421:0.007465:0.008246:0.004192:0.006119:0.004192:0.008123:0.007068:0.008965:0.005324:0.008001:0.004192
c.  Precision :@0.135266:0.490763:0.222479:0.490763:0.222479:0.477737:0.135266:0.477737:0.007068:0.003320:0.004192:0.007970:0.008567:0.005324:0.008001:0.007068:0.003702:0.006486:0.003702:0.008965:0.008659:0.004192
 :@0.455057:0.490763:0.459249:0.490763:0.459249:0.477737:0.455057:0.477737:0.004192
d.  Recall :@0.544290:0.490763:0.608370:0.490763:0.608370:0.477737:0.544290:0.477737:0.009010:0.003320:0.004192:0.004406:0.008701:0.008001:0.007068:0.007787:0.003702:0.003702:0.004192
 :@0.086957:0.518460:0.091148:0.518460:0.091148:0.505434:0.086957:0.505434:0.004192
11.  A student solved 90 out of 100 questions correctly in a multiple-choice exam. What is the error rate of the student's :@0.107502:0.518460:0.931728:0.518460:0.931728:0.505434:0.107502:0.505434:0.008246:0.008246:0.003320:0.004192:0.005370:0.009867:0.004852:0.006486:0.005186:0.008659:0.009010:0.008001:0.008659:0.005186:0.004849:0.006486:0.008965:0.003702:0.007328:0.008001:0.009010:0.004856:0.008246:0.008246:0.004856:0.008965:0.008659:0.005186:0.004851:0.008965:0.004788:0.004852:0.008246:0.008246:0.008246:0.004856:0.009010:0.008659:0.008001:0.006486:0.005186:0.003702:0.008965:0.008659:0.006486:0.004857:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.003702:0.007404:0.004836:0.003702:0.008659:0.004856:0.007787:0.004849:0.013171:0.008659:0.003702:0.005186:0.003702:0.008995:0.003702:0.008001:0.006119:0.007068:0.008659:0.008965:0.003702:0.007068:0.008001:0.004856:0.008001:0.007022:0.007787:0.013171:0.003320:0.004851:0.014288:0.008659:0.007787:0.005186:0.004848:0.003702:0.006486:0.004860:0.005186:0.008659:0.008001:0.004849:0.008001:0.005324:0.005324:0.008965:0.005324:0.004834:0.005324:0.007787:0.005186:0.008001:0.004842:0.008965:0.004788:0.004852:0.005186:0.008659:0.008001:0.004848:0.006486:0.005186:0.008659:0.009010:0.008001:0.008659:0.005186:0.003519:0.006486:0.004192
answers? :@0.136876:0.538213:0.201724:0.538213:0.201724:0.525187:0.136876:0.525187:0.007787:0.008659:0.006486:0.011060:0.008001:0.005324:0.006486:0.006853:0.004192
[CBSE Handbook]:@0.809173:0.538213:0.927548:0.538213:0.927548:0.525187:0.809173:0.525187:0.004620:0.009469:0.008766:0.008123:0.007741:0.004192:0.010862:0.007787:0.008659:0.009010:0.008995:0.008965:0.008965:0.007603:0.004620
a.  10%  :@0.135266:0.560849:0.192602:0.560849:0.192602:0.547823:0.135266:0.547823:0.007787:0.003320:0.004192:0.007251:0.008246:0.008246:0.012514:0.001591:0.004192
 :@0.455057:0.560849:0.459249:0.560849:0.459249:0.547823:0.455057:0.547823:0.004192
b.  9% :@0.544290:0.560849:0.590168:0.560849:0.590168:0.547823:0.544290:0.547823:0.008995:0.003320:0.004192:0.004421:0.008246:0.012514:0.004192
c.  8%   :@0.135266:0.584429:0.192602:0.584429:0.192602:0.571403:0.135266:0.571403:0.007068:0.003320:0.004192:0.007970:0.008246:0.012514:0.004192:0.005645:0.004192
 :@0.455057:0.584429:0.459249:0.584429:0.459249:0.571403:0.455057:0.571403:0.004192
d.  11% :@0.544290:0.584429:0.598414:0.584429:0.598414:0.571403:0.544290:0.571403:0.009010:0.003320:0.004192:0.004406:0.008246:0.008246:0.012514:0.004192
 :@0.086957:0.612126:0.091148:0.612126:0.091148:0.599100:0.086957:0.599100:0.004192
12.  Calculate the F1 score of the model, when a model correctly predicts 120 positive sentiments out of 200 positive :@0.107502:0.612126:0.896170:0.612126:0.896170:0.599100:0.107502:0.599100:0.008246:0.008246:0.003320:0.004192:0.005370:0.009469:0.007787:0.003702:0.007068:0.008659:0.003702:0.007787:0.005186:0.008001:0.004192:0.005186:0.008659:0.008001:0.004192:0.007465:0.008246:0.004192:0.006486:0.007068:0.008965:0.005324:0.008001:0.004192:0.008965:0.004788:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.003320:0.004192:0.011060:0.008659:0.008001:0.008659:0.004192:0.007787:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.003702:0.007404:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.006486:0.004192:0.008246:0.008246:0.008246:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.004192:0.006486:0.008001:0.008659:0.005186:0.003702:0.013171:0.008001:0.008659:0.005186:0.006486:0.004192:0.008965:0.008659:0.005186:0.004192:0.008965:0.004788:0.004192:0.008246:0.008246:0.008246:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.004192
instances. However, it also incorrectly predicts 40 negative sentiments as positive.:@0.136876:0.631879:0.684663:0.631879:0.684663:0.618853:0.136876:0.618853:0.003702:0.008659:0.006486:0.005186:0.007787:0.008659:0.007068:0.008001:0.006486:0.003320:0.004192:0.010862:0.008965:0.011060:0.008001:0.007328:0.008001:0.005324:0.003320:0.004192:0.003702:0.005186:0.004192:0.007787:0.003702:0.006486:0.008965:0.004192:0.003702:0.008659:0.007068:0.008965:0.005324:0.005324:0.008001:0.007068:0.005186:0.003702:0.007404:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.006486:0.004192:0.008246:0.008246:0.004192:0.008659:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.004192:0.006486:0.008001:0.008659:0.005186:0.003702:0.013171:0.008001:0.008659:0.005186:0.006486:0.004192:0.007787:0.006486:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.003320
a.  0.8   :@0.135266:0.654514:0.192602:0.654514:0.192602:0.641489:0.135266:0.641489:0.007787:0.003320:0.004192:0.007251:0.008246:0.003320:0.008246:0.004192:0.006593:0.004192
 :@0.455057:0.654514:0.459249:0.654514:0.459249:0.641489:0.455057:0.641489:0.004192
b.  0.67 :@0.544290:0.654514:0.597465:0.654514:0.597465:0.641489:0.544290:0.641489:0.008995:0.003320:0.004192:0.004421:0.008246:0.003320:0.008246:0.008246:0.004192
c.  0.72  :@0.135266:0.678095:0.192602:0.678095:0.192602:0.665069:0.135266:0.665069:0.007068:0.003320:0.004192:0.007970:0.008246:0.003320:0.008246:0.008246:0.002539:0.004192
 :@0.455057:0.678095:0.459249:0.678095:0.459249:0.665069:0.455057:0.665069:0.004192
d.  0.82 :@0.544290:0.678095:0.597465:0.678095:0.597465:0.665069:0.544290:0.665069:0.009010:0.003320:0.004192:0.004406:0.008246:0.003320:0.008246:0.008246:0.004192
B.  Fill in the blanks. :@0.086957:0.706706:0.245775:0.706706:0.245775:0.692258:0.086957:0.692258:0.010213:0.004075:0.004650:0.006831:0.008488:0.004413:0.004413:0.004413:0.004650:0.004413:0.009857:0.004650:0.006104:0.009841:0.008978:0.004650:0.010196:0.004413:0.008826:0.009857:0.008877:0.007287:0.004075:0.004650
 :@0.086957:0.729383:0.091148:0.729383:0.091148:0.716358:0.086957:0.716358:0.004192
1.  The evaluation technique that involves dividing the dataset into training and testing subsets is called :@0.115747:0.729383:0.812166:0.729383:0.812166:0.716358:0.115747:0.716358:0.008246:0.003320:0.004192:0.005370:0.008016:0.008659:0.008001:0.003808:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.003702:0.008965:0.008659:0.003805:0.005186:0.008001:0.007068:0.008659:0.008659:0.003702:0.009010:0.008659:0.008001:0.003809:0.005186:0.008659:0.007787:0.005186:0.003803:0.003702:0.008659:0.007328:0.008965:0.003702:0.007328:0.008001:0.006486:0.003817:0.009010:0.003702:0.007328:0.003702:0.009010:0.003702:0.008659:0.009010:0.003812:0.005186:0.008659:0.008001:0.003808:0.009010:0.007787:0.005178:0.007787:0.006486:0.008001:0.005186:0.003809:0.003702:0.008659:0.005186:0.008965:0.003809:0.005186:0.005324:0.007787:0.003702:0.008659:0.003702:0.008659:0.009010:0.003801:0.007787:0.008659:0.009010:0.003805:0.005186:0.008001:0.006486:0.005186:0.003702:0.008659:0.009010:0.003811:0.006486:0.008659:0.008995:0.006486:0.008001:0.005186:0.006486:0.003818:0.003702:0.006486:0.003818:0.007068:0.007787:0.003702:0.003702:0.008001:0.009010:0.004192
……….……................:@0.811779:0.729383:0.924279:0.729383:0.924279:0.716358:0.811779:0.716358:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320
.:@0.924220:0.729383:0.927539:0.729383:0.927539:0.716358:0.924220:0.716358:0.003320
 :@0.086957:0.754386:0.091148:0.754386:0.091148:0.741360:0.086957:0.741360:0.004192
2.  In an :@0.115747:0.754386:0.174224:0.754386:0.174224:0.741360:0.115747:0.741360:0.008246:0.003320:0.004192:0.005370:0.004069:0.008659:0.003985:0.007787:0.008659:0.004192
……….……................:@0.174015:0.754386:0.286515:0.754386:0.286515:0.741360:0.174015:0.741360:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320
 scenario, the model performs poorly on both training and test datasets because it is too simple :@0.286456:0.754386:0.931711:0.754386:0.931711:0.741360:0.286456:0.741360:0.003985:0.006486:0.007068:0.008001:0.008659:0.007787:0.005324:0.003702:0.008965:0.003320:0.003971:0.005186:0.008659:0.008001:0.003981:0.013171:0.008965:0.009010:0.008001:0.003702:0.003990:0.008995:0.008001:0.005324:0.004788:0.008965:0.005324:0.013171:0.006486:0.003981:0.008995:0.008965:0.008965:0.005324:0.003702:0.007404:0.003979:0.008965:0.008659:0.003982:0.008995:0.008965:0.005186:0.008659:0.003979:0.005186:0.005324:0.007787:0.003702:0.008659:0.003702:0.008659:0.009010:0.003974:0.007787:0.008659:0.009010:0.003979:0.005186:0.008001:0.006486:0.005186:0.003984:0.009010:0.007787:0.005186:0.007787:0.006486:0.008001:0.005186:0.006486:0.003981:0.008995:0.008001:0.007068:0.007787:0.008659:0.006486:0.008001:0.003981:0.003702:0.005186:0.003987:0.003702:0.006486:0.003991:0.005186:0.008965:0.008965:0.003982:0.006486:0.003702:0.013171:0.008995:0.003702:0.008001:0.004192
to capture the underlying patterns.:@0.136876:0.772904:0.371729:0.772904:0.371729:0.759879:0.136876:0.759879:0.005186:0.008965:0.004192:0.007068:0.007787:0.008995:0.005186:0.008659:0.005324:0.008001:0.004192:0.005186:0.008659:0.008001:0.004192:0.008659:0.008659:0.009010:0.008001:0.005324:0.003702:0.007404:0.003702:0.008659:0.009010:0.004192:0.008995:0.007787:0.005186:0.005186:0.008001:0.005324:0.008659:0.006486:0.003320
 :@0.086957:0.797907:0.091148:0.797907:0.091148:0.784881:0.086957:0.784881:0.004192
3.  The F1-Score is calculated as the harmonic mean of :@0.115747:0.797907:0.487563:0.797907:0.487563:0.784881:0.115747:0.784881:0.008246:0.003320:0.004192:0.005370:0.008016:0.008659:0.008001:0.004192:0.007465:0.008246:0.006119:0.008123:0.007068:0.008965:0.005324:0.008001:0.004192:0.003702:0.006486:0.004192:0.007068:0.007787:0.003702:0.007068:0.008659:0.003702:0.007787:0.005186:0.008001:0.009010:0.004192:0.007787:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.008659:0.007787:0.005324:0.013171:0.008965:0.008659:0.003702:0.007068:0.004192:0.013171:0.008001:0.007787:0.008659:0.004192:0.008965:0.004788:0.004192
……….……................:@0.487540:0.797907:0.600041:0.797907:0.600041:0.784881:0.487540:0.784881:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320
 and :@0.599981:0.797907:0.633820:0.797907:0.633820:0.784881:0.599981:0.784881:0.004192:0.007787:0.008659:0.009010:0.004192
……….……................ :@0.633811:0.797907:0.750504:0.797907:0.750504:0.784881:0.633811:0.784881:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.004192
.:@0.750443:0.797907:0.753763:0.797907:0.753763:0.784881:0.750443:0.784881:0.003320
 :@0.086957:0.822909:0.091148:0.822909:0.091148:0.809883:0.086957:0.809883:0.004192
4.  A good F1 score means that you have low false positives and low :@0.115747:0.822909:0.578983:0.822909:0.578983:0.809883:0.115747:0.809883:0.008246:0.003320:0.004192:0.005370:0.009867:0.004192:0.009010:0.008965:0.008965:0.009010:0.004192:0.007465:0.008246:0.004192:0.006486:0.007068:0.008965:0.005324:0.008001:0.004192:0.013171:0.008001:0.007787:0.008659:0.006486:0.004192:0.005186:0.008659:0.007787:0.005186:0.004192:0.007404:0.008965:0.008659:0.004192:0.008659:0.007787:0.007328:0.008001:0.004192:0.003702:0.008965:0.011060:0.004192:0.004788:0.007787:0.003702:0.006486:0.008001:0.004192:0.008995:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.006486:0.004192:0.007787:0.008659:0.009010:0.004192:0.003702:0.008965:0.011060:0.004192
……….……................:@0.578954:0.822909:0.691455:0.822909:0.691455:0.809883:0.578954:0.809883:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320
 negatives.:@0.691395:0.822909:0.763066:0.822909:0.763066:0.809883:0.691395:0.809883:0.004192:0.008659:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.006486:0.003320
 :@0.086957:0.847912:0.091148:0.847912:0.091148:0.834886:0.086957:0.834886:0.004192
5.  Accuracy is the evaluation metric used to measure the :@0.115747:0.847912:0.504681:0.847912:0.504681:0.834886:0.115747:0.834886:0.008246:0.003320:0.004192:0.005370:0.009867:0.007068:0.007068:0.008659:0.005324:0.007787:0.007068:0.007404:0.004192:0.003702:0.006486:0.004192:0.005186:0.008659:0.008001:0.004192:0.008001:0.007328:0.007787:0.003702:0.008659:0.007787:0.005186:0.003702:0.008965:0.008659:0.004192:0.013171:0.008001:0.005186:0.005324:0.003702:0.007068:0.004192:0.008659:0.006486:0.008001:0.009010:0.004192:0.005186:0.008965:0.004192:0.013171:0.008001:0.007787:0.006486:0.008659:0.005324:0.008001:0.004192:0.005186:0.008659:0.008001:0.004192
……….……................:@0.504646:0.847912:0.617147:0.847912:0.617147:0.834886:0.504646:0.834886:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320
 of predictions made by the model.:@0.617087:0.847912:0.853470:0.847912:0.853470:0.834886:0.617087:0.834886:0.004192:0.008965:0.004788:0.004192:0.008995:0.005324:0.008001:0.009010:0.003702:0.007068:0.005186:0.003702:0.008965:0.008659:0.006486:0.004192:0.013171:0.007787:0.009010:0.008001:0.004192:0.008995:0.007404:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.003320
 :@0.086957:0.872914:0.091148:0.872914:0.091148:0.859888:0.086957:0.859888:0.004192
6.  Overfitting occurs when the model is too :@0.115747:0.872914:0.416581:0.872914:0.416581:0.859888:0.115747:0.859888:0.008246:0.003320:0.004192:0.005370:0.011535:0.007328:0.008001:0.005324:0.004788:0.003702:0.005186:0.005186:0.003702:0.008659:0.009010:0.004192:0.008965:0.007068:0.007068:0.008659:0.005324:0.006486:0.004192:0.011060:0.008659:0.008001:0.008659:0.004192:0.005186:0.008659:0.008001:0.004192:0.013171:0.008965:0.009010:0.008001:0.003702:0.004192:0.003702:0.006486:0.004192:0.005186:0.008965:0.008965:0.004192
……….……................:@0.416556:0.872914:0.529057:0.872914:0.529057:0.859888:0.416556:0.859888:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320
, performing well on training data but poorly on test data.:@0.528997:0.872914:0.917824:0.872914:0.917824:0.859888:0.528997:0.859888:0.003320:0.004192:0.008995:0.008001:0.005324:0.004788:0.008965:0.005324:0.013171:0.003702:0.008659:0.009010:0.004192:0.011060:0.008001:0.003702:0.003702:0.004192:0.008965:0.008659:0.004192:0.005186:0.005324:0.007787:0.003702:0.008659:0.003702:0.008659:0.009010:0.004192:0.009010:0.007787:0.005186:0.007787:0.004192:0.008995:0.008659:0.005186:0.004192:0.008995:0.008965:0.008965:0.005324:0.003702:0.007404:0.004192:0.008965:0.008659:0.004192:0.005186:0.008001:0.006486:0.005186:0.004192:0.009010:0.007787:0.005186:0.007787:0.003320
 :@0.086957:0.896682:0.091148:0.896682:0.091148:0.883656:0.086957:0.883656:0.004192
7.  The confusion matrix consists of 4 different combinations: True Positive (TP), True Negative (TN), False Positive (FP), and :@0.115747:0.896682:0.931734:0.896682:0.931734:0.883656:0.115747:0.883656:0.008246:0.003320:0.004192:0.005370:0.008016:0.008659:0.008001:0.003647:0.007068:0.008965:0.008659:0.004788:0.008659:0.006486:0.003702:0.008965:0.008659:0.003655:0.013171:0.007787:0.005186:0.005324:0.003702:0.007022:0.003647:0.007068:0.008965:0.008659:0.006486:0.003702:0.006486:0.005186:0.006486:0.003662:0.008965:0.004788:0.003650:0.008246:0.003652:0.009010:0.003702:0.004788:0.004788:0.008001:0.005324:0.008001:0.008659:0.005186:0.003641:0.007068:0.008965:0.013171:0.008995:0.003702:0.008659:0.007787:0.005186:0.003702:0.008965:0.008659:0.006486:0.003320:0.003650:0.006685:0.005324:0.008659:0.008001:0.003644:0.008001:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.003656:0.004620:0.008016:0.008567:0.004620:0.003320:0.003641:0.006685:0.005324:0.008659:0.008001:0.003643:0.011443:0.008001:0.009010:0.007787:0.005186:0.003702:0.007328:0.008001:0.003646:0.004620:0.008016:0.011443:0.004620:0.003320:0.003639:0.006947:0.007787:0.003702:0.006486:0.008001:0.003655:0.008001:0.008965:0.006486:0.003702:0.005186:0.003702:0.007328:0.008001:0.003656:0.004620:0.007465:0.008567:0.004620:0.003320:0.003646:0.007787:0.008659:0.009010:0.004192
……….……................:@0.136876:0.916435:0.249377:0.916435:0.249377:0.903409:0.136876:0.903409:0.011213:0.011213:0.011213:0.003320:0.011213:0.011213:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320:0.003320
.:@0.249317:0.916435:0.252637:0.916435:0.252637:0.903409:0.249317:0.903409:0.003320